{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Project Documentation: Crypto Analytics BI \u00b6 Welcome to the detailed documentation for the Crypto Analytics BI On-Premise ELT Pipeline. This documentation provides in-depth information about the project's architecture, data pipeline, data modeling strategy, and development guidelines. Navigation \u00b6 Architecture Overview : Understand the overall system design and component interactions. Data Sources : Learn about the external APIs used and the data they provide. Ingestion Pipeline : Details on how data is extracted and loaded into PostgreSQL using Airflow. dbt Data Modeling : Explore how raw data is transformed into an analytical model. Dimensional Model Sources Staging Layer Marts Layer Testing and Quality Airflow Orchestration : Specifics of DAG design and pipeline scheduling. Power BI Integration : Guidelines for connecting and visualizing data. Development Guide : Information for developers contributing to the project. Troubleshooting : Solutions for common problems. Getting Started \u00b6 If you haven't already, please review the main Project README.md for setup and installation instructions.","title":"Home"},{"location":"#project-documentation-crypto-analytics-bi","text":"Welcome to the detailed documentation for the Crypto Analytics BI On-Premise ELT Pipeline. This documentation provides in-depth information about the project's architecture, data pipeline, data modeling strategy, and development guidelines.","title":"Project Documentation: Crypto Analytics BI"},{"location":"#navigation","text":"Architecture Overview : Understand the overall system design and component interactions. Data Sources : Learn about the external APIs used and the data they provide. Ingestion Pipeline : Details on how data is extracted and loaded into PostgreSQL using Airflow. dbt Data Modeling : Explore how raw data is transformed into an analytical model. Dimensional Model Sources Staging Layer Marts Layer Testing and Quality Airflow Orchestration : Specifics of DAG design and pipeline scheduling. Power BI Integration : Guidelines for connecting and visualizing data. Development Guide : Information for developers contributing to the project. Troubleshooting : Solutions for common problems.","title":"Navigation"},{"location":"#getting-started","text":"If you haven't already, please review the main Project README.md for setup and installation instructions.","title":"Getting Started"},{"location":"01_architecture/","text":"System Architecture \u00b6 This project implements an on-premise Extract, Load, Transform (ELT) pipeline designed for analyzing cryptocurrency market data and blockchain activity. The architecture prioritizes modularity, scalability within an on-premise context, and the use of open-source technologies. Core Components \u00b6 External Data Sources: Blockchair API: Provides detailed blockchain data including blocks, transactions, addresses, inputs, and outputs for Bitcoin (BTC), Ethereum (ETH), and Dogecoin (DOGE). CoinGecko API: Supplies market data such as historical prices, trading volumes, and market capitalizations for the target cryptocurrencies. Data Ingestion (Python & Airflow): Custom Python scripts located in src/ingestion/ are responsible for interacting with the external APIs. These scripts handle API request logic, basic data parsing (e.g., JSON to Python dictionaries/lists), and initial data structuring. Apache Airflow DAGs orchestrate the execution of these ingestion scripts, scheduling them to run periodically (e.g., daily). Apache Airflow (Orchestration Engine): Serves as the central nervous system of the ELT pipeline. Manages the scheduling, execution, and monitoring of all pipeline tasks. Tasks include triggering ingestion scripts, executing dbt transformations, and performing data quality checks. Uses a PostgreSQL database ( postgres_airflow_meta ) for its own metadata storage. PostgreSQL (Data Warehouse): A robust open-source relational database serving as the central data repository ( postgres_app_db ). Raw/Staging Schemas (e.g., public , raw_data ): This area holds the data as it's initially loaded from the APIs by the ingestion scripts. Data here is largely unaltered, providing a record of what was received. Analytical Schemas (e.g., staging , intermediate , analytics , marts ): These schemas house the cleaned, transformed, and modeled data produced by dbt. This is where the dimensional model (facts and dimensions) resides, optimized for analytical queries. dbt (Data Build Tool): The primary tool for transforming data within the PostgreSQL data warehouse. dbt projects are defined in the dbt_project/ directory. Utilizes SQL (primarily) to define data models, tests, and documentation. Reads from the raw/staging schemas and writes transformed data into the analytical schemas. Enforces data quality through automated testing. Power BI Desktop (Business Intelligence & Visualization): The end-user tool for data analysis and visualization. Connects directly to the analytical schemas in the postgres_app_db (e.g., analytics.fct_transactions ). Enables the creation of interactive dashboards, reports, and ad-hoc queries based on the transformed data. Data Flow \u00b6 The data flows through the system in the following sequence: Extract: Airflow schedules and triggers Python ingestion scripts. These scripts call external APIs (Blockchair, CoinGecko) to fetch the latest data. Load: The Python scripts load the raw, unprocessed data into designated tables within the raw_data (or public ) schema of the postgres_app_db . Transform: Airflow then triggers a sequence of dbt commands: dbt seed (if applicable) loads static data. dbt run executes the SQL models defined in the dbt_project/ . This involves: Cleaning and standardizing raw data into staging models. Performing intermediate calculations or joins in intermediate models (if any). Building the final fact and dimension tables in the marts (or analytics ) schema. dbt test runs data quality tests defined on the models to ensure accuracy and integrity. Analyze & Visualize: Power BI connects to the marts schema in postgres_app_db , allowing users to query the transformed data and build visualizations. Deployment (Docker Compose) \u00b6 All backend services (PostgreSQL instances, Airflow components) are containerized using Docker Compose. This approach provides: * Consistency: Ensures the development, testing, and (potentially) production environments are similar. * Isolation: Services run in isolated containers, preventing conflicts. * Ease of Setup: Simplifies the process of getting the entire backend stack running with a single docker-compose up command. * Portability: The setup can be easily shared and run on any machine with Docker installed. Refer to the docker-compose.yaml file for the specific service definitions and configurations. Diagram \u00b6 graph TD subgraph \"External Systems\" API_Blockchair[Blockchair API] API_CoinGecko[CoinGecko API] end subgraph \"Ingestion Layer (Python)\" IngestScripts[src/ingestion/*.py] end subgraph \"Orchestration (Apache Airflow)\" AirflowDAGs[dags/*.py] -- Triggers --> IngestScripts AirflowDAGs -- Triggers --> DBTCommands[dbt run, dbt test, etc.] AirflowCore[Airflow Scheduler & Webserver] -- Uses --> AirflowMetaDB[(Postgres - Airflow Meta)] end subgraph \"Data Warehouse (PostgreSQL - App DB)\" direction LR RawSchema[Raw/Staging Schemas e.g., 'public'] StagingSchema[dbt Staging Schema e.g., 'staging'] IntermediateSchema[dbt Intermediate Schema e.g., 'intermediate'] MartsSchema[dbt Marts Schema e.g., 'analytics'] RawSchema --> StagingSchema StagingSchema --> IntermediateSchema IntermediateSchema --> MartsSchema end subgraph \"Transformation (dbt)\" DBTProject[dbt_project/] -- Defines Models & Tests --> DBTCommands end subgraph \"Business Intelligence\" PowerBI[Power BI Desktop] end API_Blockchair --> IngestScripts API_CoinGecko --> IngestScripts IngestScripts -- Loads Raw Data --> RawSchema DBTCommands -- Reads/Writes --> StagingSchema DBTCommands -- Reads/Writes --> IntermediateSchema DBTCommands -- Reads/Writes --> MartsSchema MartsSchema -- Queried By --> PowerBI style AirflowCore fill:#f9f,stroke:#333,stroke-width:2px style DBTProject fill:#ccf,stroke:#333,stroke-width:2px style PowerBI fill:#9cf,stroke:#333,stroke-width:2px (This Mermaid diagram provides a visual representation. You can generate an image from it.)","title":"System Architecture"},{"location":"01_architecture/#system-architecture","text":"This project implements an on-premise Extract, Load, Transform (ELT) pipeline designed for analyzing cryptocurrency market data and blockchain activity. The architecture prioritizes modularity, scalability within an on-premise context, and the use of open-source technologies.","title":"System Architecture"},{"location":"01_architecture/#core-components","text":"External Data Sources: Blockchair API: Provides detailed blockchain data including blocks, transactions, addresses, inputs, and outputs for Bitcoin (BTC), Ethereum (ETH), and Dogecoin (DOGE). CoinGecko API: Supplies market data such as historical prices, trading volumes, and market capitalizations for the target cryptocurrencies. Data Ingestion (Python & Airflow): Custom Python scripts located in src/ingestion/ are responsible for interacting with the external APIs. These scripts handle API request logic, basic data parsing (e.g., JSON to Python dictionaries/lists), and initial data structuring. Apache Airflow DAGs orchestrate the execution of these ingestion scripts, scheduling them to run periodically (e.g., daily). Apache Airflow (Orchestration Engine): Serves as the central nervous system of the ELT pipeline. Manages the scheduling, execution, and monitoring of all pipeline tasks. Tasks include triggering ingestion scripts, executing dbt transformations, and performing data quality checks. Uses a PostgreSQL database ( postgres_airflow_meta ) for its own metadata storage. PostgreSQL (Data Warehouse): A robust open-source relational database serving as the central data repository ( postgres_app_db ). Raw/Staging Schemas (e.g., public , raw_data ): This area holds the data as it's initially loaded from the APIs by the ingestion scripts. Data here is largely unaltered, providing a record of what was received. Analytical Schemas (e.g., staging , intermediate , analytics , marts ): These schemas house the cleaned, transformed, and modeled data produced by dbt. This is where the dimensional model (facts and dimensions) resides, optimized for analytical queries. dbt (Data Build Tool): The primary tool for transforming data within the PostgreSQL data warehouse. dbt projects are defined in the dbt_project/ directory. Utilizes SQL (primarily) to define data models, tests, and documentation. Reads from the raw/staging schemas and writes transformed data into the analytical schemas. Enforces data quality through automated testing. Power BI Desktop (Business Intelligence & Visualization): The end-user tool for data analysis and visualization. Connects directly to the analytical schemas in the postgres_app_db (e.g., analytics.fct_transactions ). Enables the creation of interactive dashboards, reports, and ad-hoc queries based on the transformed data.","title":"Core Components"},{"location":"01_architecture/#data-flow","text":"The data flows through the system in the following sequence: Extract: Airflow schedules and triggers Python ingestion scripts. These scripts call external APIs (Blockchair, CoinGecko) to fetch the latest data. Load: The Python scripts load the raw, unprocessed data into designated tables within the raw_data (or public ) schema of the postgres_app_db . Transform: Airflow then triggers a sequence of dbt commands: dbt seed (if applicable) loads static data. dbt run executes the SQL models defined in the dbt_project/ . This involves: Cleaning and standardizing raw data into staging models. Performing intermediate calculations or joins in intermediate models (if any). Building the final fact and dimension tables in the marts (or analytics ) schema. dbt test runs data quality tests defined on the models to ensure accuracy and integrity. Analyze & Visualize: Power BI connects to the marts schema in postgres_app_db , allowing users to query the transformed data and build visualizations.","title":"Data Flow"},{"location":"01_architecture/#deployment-docker-compose","text":"All backend services (PostgreSQL instances, Airflow components) are containerized using Docker Compose. This approach provides: * Consistency: Ensures the development, testing, and (potentially) production environments are similar. * Isolation: Services run in isolated containers, preventing conflicts. * Ease of Setup: Simplifies the process of getting the entire backend stack running with a single docker-compose up command. * Portability: The setup can be easily shared and run on any machine with Docker installed. Refer to the docker-compose.yaml file for the specific service definitions and configurations.","title":"Deployment (Docker Compose)"},{"location":"01_architecture/#diagram","text":"graph TD subgraph \"External Systems\" API_Blockchair[Blockchair API] API_CoinGecko[CoinGecko API] end subgraph \"Ingestion Layer (Python)\" IngestScripts[src/ingestion/*.py] end subgraph \"Orchestration (Apache Airflow)\" AirflowDAGs[dags/*.py] -- Triggers --> IngestScripts AirflowDAGs -- Triggers --> DBTCommands[dbt run, dbt test, etc.] AirflowCore[Airflow Scheduler & Webserver] -- Uses --> AirflowMetaDB[(Postgres - Airflow Meta)] end subgraph \"Data Warehouse (PostgreSQL - App DB)\" direction LR RawSchema[Raw/Staging Schemas e.g., 'public'] StagingSchema[dbt Staging Schema e.g., 'staging'] IntermediateSchema[dbt Intermediate Schema e.g., 'intermediate'] MartsSchema[dbt Marts Schema e.g., 'analytics'] RawSchema --> StagingSchema StagingSchema --> IntermediateSchema IntermediateSchema --> MartsSchema end subgraph \"Transformation (dbt)\" DBTProject[dbt_project/] -- Defines Models & Tests --> DBTCommands end subgraph \"Business Intelligence\" PowerBI[Power BI Desktop] end API_Blockchair --> IngestScripts API_CoinGecko --> IngestScripts IngestScripts -- Loads Raw Data --> RawSchema DBTCommands -- Reads/Writes --> StagingSchema DBTCommands -- Reads/Writes --> IntermediateSchema DBTCommands -- Reads/Writes --> MartsSchema MartsSchema -- Queried By --> PowerBI style AirflowCore fill:#f9f,stroke:#333,stroke-width:2px style DBTProject fill:#ccf,stroke:#333,stroke-width:2px style PowerBI fill:#9cf,stroke:#333,stroke-width:2px (This Mermaid diagram provides a visual representation. You can generate an image from it.)","title":"Diagram"},{"location":"02_data_sources/","text":"Data Sources \u00b6 This project relies on external APIs to gather blockchain activity and cryptocurrency market data. The primary sources are the Blockchair API and the CoinGecko API. 1. Blockchair API \u00b6 Website: https://blockchair.com/api/docs Purpose: Provides comprehensive data for various blockchains. In this project, it's used for Bitcoin (BTC), Ethereum (ETH), and Dogecoin (DOGE). Data Points Extracted (Examples): Blocks: Block ID / Height Block Hash Timestamp Transaction Count Block Size Difficulty Miner information (if available) Transactions: Transaction Hash (ID) Block ID (confirming block) Timestamp Fees (in native currency and USD) Input Count & Output Count Total Input Value & Total Output Value (in native currency and USD) Transaction Size Coinbase status Input addresses and values (potentially, for more detailed analysis) Output addresses and values (potentially, for more detailed analysis) Addresses (Potential Future Enhancement): Address balance Transaction count associated with an address First seen / Last seen timestamps Endpoints Used (Examples): /<coin>/blocks?date=<YYYY-MM-DD>&limit=<N> : To fetch blocks mined on a specific date. /<coin>/transactions?date=<YYYY-MM-DD>&limit=<N> : To fetch transactions confirmed on a specific date. More specific endpoints might be used for fetching details of a single block or transaction if needed. Authentication: Blockchair offers a generous free tier. For higher rate limits or more features, an API key might be required and can be configured via environment variables. Rate Limits: The free tier has rate limits (e.g., requests per minute/hour). Ingestion scripts include basic delays ( time.sleep() ) to respect these limits. For high-volume backfills, more sophisticated rate limiting and retry logic might be necessary. Data Format: JSON. Ingestion Script: src/ingestion/blockchair_ingestor.py 2. CoinGecko API \u00b6 Website: https://www.coingecko.com/en/api/documentation Purpose: Provides cryptocurrency market data, including prices, trading volumes, market capitalization, and historical data. Data Points Extracted (Examples for BTC, ETH, DOGE): Historical Price: Daily closing price in USD. Total Volume: Daily total trading volume in USD across tracked exchanges. Market Capitalization: Daily market cap in USD. Endpoints Used (Examples): /coins/{id}/history?date={dd-mm-yyyy}&localization=false : To fetch historical market data (price, volume, market cap) for a specific coin on a specific date. The id corresponds to CoinGecko's unique identifier for the coin (e.g., \"bitcoin\", \"ethereum\", \"dogecoin\"). Authentication: Public API: Available without an API key, but with stricter rate limits. Pro API: Requires an API key (configured via COINGECKO_API_KEY environment variable) for higher rate limits and additional features. The ingestion script supports using the Pro API if an key is provided. Rate Limits: The public API has rate limits (e.g., 10-30 calls per minute). The Pro API offers significantly higher limits. Ingestion scripts include basic delays ( time.sleep() ) between calls. Data Format: JSON. Ingestion Script: src/ingestion/coingecko_ingestor.py Data Granularity and Update Frequency \u00b6 Blockchain Data (Blockchair): Aiming for daily granularity, fetching all blocks and transactions confirmed on the previous day. Market Data (CoinGecko): Daily granularity, fetching the previous day's closing price, total volume, and market cap. The Airflow pipeline is typically scheduled to run once daily to update the data warehouse with the previous day's information. ```","title":"Data Sources"},{"location":"02_data_sources/#data-sources","text":"This project relies on external APIs to gather blockchain activity and cryptocurrency market data. The primary sources are the Blockchair API and the CoinGecko API.","title":"Data Sources"},{"location":"02_data_sources/#1-blockchair-api","text":"Website: https://blockchair.com/api/docs Purpose: Provides comprehensive data for various blockchains. In this project, it's used for Bitcoin (BTC), Ethereum (ETH), and Dogecoin (DOGE). Data Points Extracted (Examples): Blocks: Block ID / Height Block Hash Timestamp Transaction Count Block Size Difficulty Miner information (if available) Transactions: Transaction Hash (ID) Block ID (confirming block) Timestamp Fees (in native currency and USD) Input Count & Output Count Total Input Value & Total Output Value (in native currency and USD) Transaction Size Coinbase status Input addresses and values (potentially, for more detailed analysis) Output addresses and values (potentially, for more detailed analysis) Addresses (Potential Future Enhancement): Address balance Transaction count associated with an address First seen / Last seen timestamps Endpoints Used (Examples): /<coin>/blocks?date=<YYYY-MM-DD>&limit=<N> : To fetch blocks mined on a specific date. /<coin>/transactions?date=<YYYY-MM-DD>&limit=<N> : To fetch transactions confirmed on a specific date. More specific endpoints might be used for fetching details of a single block or transaction if needed. Authentication: Blockchair offers a generous free tier. For higher rate limits or more features, an API key might be required and can be configured via environment variables. Rate Limits: The free tier has rate limits (e.g., requests per minute/hour). Ingestion scripts include basic delays ( time.sleep() ) to respect these limits. For high-volume backfills, more sophisticated rate limiting and retry logic might be necessary. Data Format: JSON. Ingestion Script: src/ingestion/blockchair_ingestor.py","title":"1. Blockchair API"},{"location":"02_data_sources/#2-coingecko-api","text":"Website: https://www.coingecko.com/en/api/documentation Purpose: Provides cryptocurrency market data, including prices, trading volumes, market capitalization, and historical data. Data Points Extracted (Examples for BTC, ETH, DOGE): Historical Price: Daily closing price in USD. Total Volume: Daily total trading volume in USD across tracked exchanges. Market Capitalization: Daily market cap in USD. Endpoints Used (Examples): /coins/{id}/history?date={dd-mm-yyyy}&localization=false : To fetch historical market data (price, volume, market cap) for a specific coin on a specific date. The id corresponds to CoinGecko's unique identifier for the coin (e.g., \"bitcoin\", \"ethereum\", \"dogecoin\"). Authentication: Public API: Available without an API key, but with stricter rate limits. Pro API: Requires an API key (configured via COINGECKO_API_KEY environment variable) for higher rate limits and additional features. The ingestion script supports using the Pro API if an key is provided. Rate Limits: The public API has rate limits (e.g., 10-30 calls per minute). The Pro API offers significantly higher limits. Ingestion scripts include basic delays ( time.sleep() ) between calls. Data Format: JSON. Ingestion Script: src/ingestion/coingecko_ingestor.py","title":"2. CoinGecko API"},{"location":"02_data_sources/#data-granularity-and-update-frequency","text":"Blockchain Data (Blockchair): Aiming for daily granularity, fetching all blocks and transactions confirmed on the previous day. Market Data (CoinGecko): Daily granularity, fetching the previous day's closing price, total volume, and market cap. The Airflow pipeline is typically scheduled to run once daily to update the data warehouse with the previous day's information. ```","title":"Data Granularity and Update Frequency"},{"location":"03_ingestion_pipeline/","text":"Data Ingestion Pipeline \u00b6 The data ingestion pipeline is responsible for extracting data from external APIs (Blockchair and CoinGecko) and loading it into raw/staging tables in the PostgreSQL application database. This process is orchestrated by Apache Airflow. Core Components \u00b6 Python Ingestion Scripts ( src/ingestion/ ): blockchair_ingestor.py : Handles all interactions with the Blockchair API. Fetches block data for specified cryptocurrencies (BTC, ETH, DOGE) for a given date. Fetches transaction data for these cryptocurrencies for a given date. Includes functions to create necessary raw tables in PostgreSQL if they don't exist ( CREATE TABLE IF NOT EXISTS ). Uses psycopg2 to connect to PostgreSQL and insert data, handling potential conflicts (e.g., ON CONFLICT DO NOTHING for idempotent writes). coingecko_ingestor.py : Handles all interactions with the CoinGecko API. Fetches historical market data (price, volume, market cap) for specified cryptocurrencies for a given date. Creates the raw market data table in PostgreSQL if it doesn't exist. Inserts or updates market data, using ON CONFLICT DO UPDATE to ensure the latest data for a given coin and date is stored. db_utils.py : Contains utility functions for establishing PostgreSQL database connections and executing queries (single, batch). Reads database credentials from environment variables. Shared Utilities ( src/common/utils.py ): Provides common helper functions, such as date manipulation (e.g., getting yesterday's date in various formats). Apache Airflow DAGs ( dags/ ): A primary DAG (e.g., crypto_data_ingestion_vX.py ) orchestrates the entire ingestion process. Scheduling: The DAG is typically scheduled to run daily (e.g., @daily or a cron expression like 0 1 * * * to run at 1 AM UTC). Tasks: PythonOperator tasks are used to call the main functions within the ingestion scripts (e.g., ingest_recent_blocks , ingest_transactions_for_blocks , ingest_coingecko_data_for_date ). Tasks can be defined to run in parallel for different data sources or sequentially if there are dependencies. Error handling, retries, and logging are managed by Airflow. Workflow for a Single Daily Run \u00b6 Trigger: Airflow's scheduler triggers the main ingestion DAG at the scheduled time. Parameterization (Date): The DAG typically processes data for the \"previous day\" relative to its execution date. Utility functions help determine this target date. CoinGecko Ingestion Task: The ingest_coingecko_data_for_date function is called. It iterates through the target cryptocurrencies (BTC, ETH, DOGE). For each coin, it calls the CoinGecko API to fetch market data for the target date. A small delay is introduced between API calls to respect rate limits. Fetched data is then inserted/updated into the raw_market_prices_volumes table in PostgreSQL. Blockchair Ingestion Tasks (can run in parallel with CoinGecko or sequentially): The ingest_all_blockchair_data_callable (or similar wrapper function in the DAG) is called. This function iterates through BTC, ETH, and DOGE. For each coin: ingest_recent_blocks fetches block data for the target date and inserts it into raw_blockchain_blocks . ingest_transactions_for_blocks fetches transaction data for the target date (or based on fetched block IDs if implemented that way) and inserts it into raw_blockchain_transactions . Rate limiting delays are also applied here. Logging and Monitoring: All ingestion steps log their progress, successes, and failures. Airflow captures these logs, making them accessible via the Airflow UI. Airflow monitors task status (running, success, failed) and handles retries as configured. Completion: Once all ingestion tasks complete successfully, the raw data for the target day is available in PostgreSQL, ready for dbt transformation. Database Interaction \u00b6 Schema for Raw Data: Ingested data is loaded into tables within the public schema (or a dedicated raw_data schema if preferred) in the postgres_app_db . Table Creation: Scripts use CREATE TABLE IF NOT EXISTS to ensure tables are present. Idempotency: For blockchain data (blocks, transactions), ON CONFLICT (primary_key_or_unique_constraint) DO NOTHING is used. This ensures that if a record already exists (e.g., due to a re-run), it's not duplicated. For market data, ON CONFLICT (coin_id, price_date) DO UPDATE SET ... is used. This allows for corrections or updates to historical market data if the source provides them. Configuration \u00b6 API Keys: Optional API keys (e.g., for CoinGecko Pro) are managed via environment variables (e.g., COINGECKO_API_KEY ). Database Credentials: Connection details for PostgreSQL ( DB_HOST , DB_PORT , DB_NAME , DB_USER_APP , DB_PASSWORD_APP ) are sourced from environment variables, which are loaded by Docker Compose into the Airflow worker/scheduler environment. Target Cryptocurrencies: The list of coins to process (BTC, ETH, DOGE) is typically hardcoded or configurable within the DAG or ingestion scripts. Error Handling and Retries \u00b6 API Request Errors: requests library's exception handling ( response.raise_for_status() , try-except blocks) is used to catch HTTP errors or network issues. Database Errors: psycopg2 exceptions are caught during database operations. Airflow Retries: DAG tasks are configured with a retry policy (e.g., retry once after a 5-minute delay) to handle transient failures. Logging: Comprehensive logging helps in diagnosing issues after a failure. ```","title":"Ingestion Pipeline"},{"location":"03_ingestion_pipeline/#data-ingestion-pipeline","text":"The data ingestion pipeline is responsible for extracting data from external APIs (Blockchair and CoinGecko) and loading it into raw/staging tables in the PostgreSQL application database. This process is orchestrated by Apache Airflow.","title":"Data Ingestion Pipeline"},{"location":"03_ingestion_pipeline/#core-components","text":"Python Ingestion Scripts ( src/ingestion/ ): blockchair_ingestor.py : Handles all interactions with the Blockchair API. Fetches block data for specified cryptocurrencies (BTC, ETH, DOGE) for a given date. Fetches transaction data for these cryptocurrencies for a given date. Includes functions to create necessary raw tables in PostgreSQL if they don't exist ( CREATE TABLE IF NOT EXISTS ). Uses psycopg2 to connect to PostgreSQL and insert data, handling potential conflicts (e.g., ON CONFLICT DO NOTHING for idempotent writes). coingecko_ingestor.py : Handles all interactions with the CoinGecko API. Fetches historical market data (price, volume, market cap) for specified cryptocurrencies for a given date. Creates the raw market data table in PostgreSQL if it doesn't exist. Inserts or updates market data, using ON CONFLICT DO UPDATE to ensure the latest data for a given coin and date is stored. db_utils.py : Contains utility functions for establishing PostgreSQL database connections and executing queries (single, batch). Reads database credentials from environment variables. Shared Utilities ( src/common/utils.py ): Provides common helper functions, such as date manipulation (e.g., getting yesterday's date in various formats). Apache Airflow DAGs ( dags/ ): A primary DAG (e.g., crypto_data_ingestion_vX.py ) orchestrates the entire ingestion process. Scheduling: The DAG is typically scheduled to run daily (e.g., @daily or a cron expression like 0 1 * * * to run at 1 AM UTC). Tasks: PythonOperator tasks are used to call the main functions within the ingestion scripts (e.g., ingest_recent_blocks , ingest_transactions_for_blocks , ingest_coingecko_data_for_date ). Tasks can be defined to run in parallel for different data sources or sequentially if there are dependencies. Error handling, retries, and logging are managed by Airflow.","title":"Core Components"},{"location":"03_ingestion_pipeline/#workflow-for-a-single-daily-run","text":"Trigger: Airflow's scheduler triggers the main ingestion DAG at the scheduled time. Parameterization (Date): The DAG typically processes data for the \"previous day\" relative to its execution date. Utility functions help determine this target date. CoinGecko Ingestion Task: The ingest_coingecko_data_for_date function is called. It iterates through the target cryptocurrencies (BTC, ETH, DOGE). For each coin, it calls the CoinGecko API to fetch market data for the target date. A small delay is introduced between API calls to respect rate limits. Fetched data is then inserted/updated into the raw_market_prices_volumes table in PostgreSQL. Blockchair Ingestion Tasks (can run in parallel with CoinGecko or sequentially): The ingest_all_blockchair_data_callable (or similar wrapper function in the DAG) is called. This function iterates through BTC, ETH, and DOGE. For each coin: ingest_recent_blocks fetches block data for the target date and inserts it into raw_blockchain_blocks . ingest_transactions_for_blocks fetches transaction data for the target date (or based on fetched block IDs if implemented that way) and inserts it into raw_blockchain_transactions . Rate limiting delays are also applied here. Logging and Monitoring: All ingestion steps log their progress, successes, and failures. Airflow captures these logs, making them accessible via the Airflow UI. Airflow monitors task status (running, success, failed) and handles retries as configured. Completion: Once all ingestion tasks complete successfully, the raw data for the target day is available in PostgreSQL, ready for dbt transformation.","title":"Workflow for a Single Daily Run"},{"location":"03_ingestion_pipeline/#database-interaction","text":"Schema for Raw Data: Ingested data is loaded into tables within the public schema (or a dedicated raw_data schema if preferred) in the postgres_app_db . Table Creation: Scripts use CREATE TABLE IF NOT EXISTS to ensure tables are present. Idempotency: For blockchain data (blocks, transactions), ON CONFLICT (primary_key_or_unique_constraint) DO NOTHING is used. This ensures that if a record already exists (e.g., due to a re-run), it's not duplicated. For market data, ON CONFLICT (coin_id, price_date) DO UPDATE SET ... is used. This allows for corrections or updates to historical market data if the source provides them.","title":"Database Interaction"},{"location":"03_ingestion_pipeline/#configuration","text":"API Keys: Optional API keys (e.g., for CoinGecko Pro) are managed via environment variables (e.g., COINGECKO_API_KEY ). Database Credentials: Connection details for PostgreSQL ( DB_HOST , DB_PORT , DB_NAME , DB_USER_APP , DB_PASSWORD_APP ) are sourced from environment variables, which are loaded by Docker Compose into the Airflow worker/scheduler environment. Target Cryptocurrencies: The list of coins to process (BTC, ETH, DOGE) is typically hardcoded or configurable within the DAG or ingestion scripts.","title":"Configuration"},{"location":"03_ingestion_pipeline/#error-handling-and-retries","text":"API Request Errors: requests library's exception handling ( response.raise_for_status() , try-except blocks) is used to catch HTTP errors or network issues. Database Errors: psycopg2 exceptions are caught during database operations. Airflow Retries: DAG tasks are configured with a retry policy (e.g., retry once after a 5-minute delay) to handle transient failures. Logging: Comprehensive logging helps in diagnosing issues after a failure. ```","title":"Error Handling and Retries"},{"location":"05_airflow_orchestration/","text":"Apache Airflow Orchestration \u00b6 Apache Airflow is the backbone of this ELT pipeline, responsible for scheduling, executing, monitoring, and managing dependencies between all data processing tasks. This document outlines how Airflow is used in this project. Core Airflow Concepts Utilized \u00b6 DAGs (Directed Acyclic Graphs): The primary way workflows are defined in Airflow. Each DAG represents a sequence of tasks with defined dependencies. Our main DAG (e.g., crypto_data_ingestion_pipeline_vX.py in the dags/ folder) orchestrates the entire daily ELT process. Definition: Written in Python. Scheduling: Uses cron expressions or timedelta objects to define how often the DAG should run (e.g., daily at 1 AM UTC). Operators: Building blocks of DAGs; they define a single unit of work. PythonOperator : Used to execute Python callable functions. This is how our custom ingestion scripts ( src/ingestion/*.py ) are invoked. BashOperator : Used to execute shell commands. This is the primary way dbt commands ( dbt run , dbt test , dbt seed , dbt deps ) are executed. PostgresOperator (Potentially): Could be used to execute specific SQL statements directly against PostgreSQL if needed, though most SQL transformations are handled by dbt. Tasks: An instantiated Operator within a DAG becomes a Task. Tasks have upstream and downstream dependencies, defining the order of execution. Scheduler: The Airflow component that monitors all DAGs and Triggers Task Instances whose dependencies have been met and schedule has been reached. Webserver (UI): Provides a user interface for monitoring DAG runs, task statuses, logs, and managing Airflow configurations. Accessible at http://localhost:8080 in our Dockerized setup. Connections & Variables (Managed by Airflow): While our current setup primarily uses environment variables (passed via Docker Compose) for database credentials needed by dbt and Python scripts, Airflow has its own system for managing connections (e.g., to PostgreSQL) and variables. For more complex setups, these could be utilized. Main ELT DAG Structure ( crypto_data_ingestion_pipeline_vX.py ) \u00b6 The primary DAG typically has the following structure and flow: DAG Definition: dag_id , start_date , schedule_interval (e.g., '@daily' ), catchup=False (usually for production), default arguments (retries, email on failure, etc.). Task Definitions: Start Task (Optional DummyOperator ): Marks the beginning of the DAG. Ingestion Tasks (Parallel): A PythonOperator task for CoinGecko data ingestion (e.g., task_ingest_coingecko ). Calls a Python function like ingest_all_coingecko_data_callable() which in turn calls the core logic in src/ingestion/coingecko_ingestor.py . A PythonOperator task for Blockchair data ingestion (e.g., task_ingest_blockchair ). Calls a Python function like ingest_all_blockchair_data_callable() which then calls functions in src/ingestion/blockchair_ingestor.py for blocks and transactions. These two main ingestion tasks can often run in parallel as they deal with different data sources. dbt Pre-Transformation Tasks (Sequential): task_dbt_deps ( BashOperator ): Executes dbt deps --project-dir ... to install any dbt package dependencies. This should run before any other dbt command if packages are used. task_dbt_seed ( BashOperator ): Executes dbt seed --project-dir ... to load static data from CSV files in the seeds/ directory. dbt Transformation Task: task_dbt_run ( BashOperator ): Executes dbt run --project-dir ... to materialize all dbt models (staging, intermediate, marts). This is the core transformation step. dbt Testing Task: task_dbt_test ( BashOperator ): Executes dbt test --project-dir ... to run all defined data quality tests on the transformed models. End Task (Optional DummyOperator ): Marks the successful completion of the DAG. Task Dependencies (Defining the Flow): Using >> (set downstream) and << (set upstream) operators or lists for parallel execution. # Example Dependency Flow start_task = DummyOperator ( task_id = 'start' , dag = dag ) end_task = DummyOperator ( task_id = 'end' , dag = dag ) # Ingestion tasks can run in parallel ingestion_tasks = [ task_ingest_coingecko , task_ingest_blockchair ] start_task >> ingestion_tasks # Both ingestion tasks depend on start # dbt tasks run sequentially after ingestion # Ensure all ingestion tasks are complete before starting dbt sequence for task in ingestion_tasks : task >> task_dbt_deps task_dbt_deps >> task_dbt_seed >> task_dbt_run >> task_dbt_test >> end_task Configuration and Environment \u00b6 PYTHONPATH: The Airflow workers/scheduler need access to the src/ directory to import Python ingestion modules. The Docker setup mounts src/ and the DAG file often includes logic to add the project root to sys.path . DBT_PROJECT_DIR & DBT_PROFILES_DIR: The BashOperator tasks executing dbt commands need to specify the --project-dir to point to the mounted dbt_project/ directory. DBT_PROFILES_DIR (or the default ~/.dbt/ ) must contain a profiles.yml configured to connect to the postgres_app_db service, using environment variables for credentials. Environment Variables for dbt: The BashOperator tasks for dbt pass necessary database connection environment variables ( DB_HOST , DB_USER_DBT , DB_PASSWORD_DBT , etc.) which are then used by the profiles.yml env_var() function. These are sourced from the .env file by Docker Compose. Logging and Monitoring \u00b6 Task Logs: All stdout and stderr from Python scripts and dbt commands are captured by Airflow and accessible per task instance in the Airflow UI. This is crucial for debugging. DAG Runs View: Shows the status of all DAG runs (success, running, failed). Graph View / Gantt Chart: Visualize task dependencies and execution times. Alerting (Optional): Airflow can be configured to send email alerts on task failures or SLA misses (though not implemented in the basic setup). Idempotency \u00b6 DAGs: Airflow DAGs themselves are generally idempotent in terms of definition. Tasks: Ingestion scripts aim for idempotency (e.g., ON CONFLICT DO NOTHING or ON CONFLICT DO UPDATE ). dbt run is generally idempotent for view and table materializations (it recreates or replaces them). For incremental models, dbt handles idempotency based on the incremental strategy and unique keys. dbt seed can be made idempotent using flags like --full-refresh or by designing seeds to handle existing data. By leveraging Airflow, the project achieves a robust, automated, and monitorable ELT pipeline.","title":"Airflow Orchestration"},{"location":"05_airflow_orchestration/#apache-airflow-orchestration","text":"Apache Airflow is the backbone of this ELT pipeline, responsible for scheduling, executing, monitoring, and managing dependencies between all data processing tasks. This document outlines how Airflow is used in this project.","title":"Apache Airflow Orchestration"},{"location":"05_airflow_orchestration/#core-airflow-concepts-utilized","text":"DAGs (Directed Acyclic Graphs): The primary way workflows are defined in Airflow. Each DAG represents a sequence of tasks with defined dependencies. Our main DAG (e.g., crypto_data_ingestion_pipeline_vX.py in the dags/ folder) orchestrates the entire daily ELT process. Definition: Written in Python. Scheduling: Uses cron expressions or timedelta objects to define how often the DAG should run (e.g., daily at 1 AM UTC). Operators: Building blocks of DAGs; they define a single unit of work. PythonOperator : Used to execute Python callable functions. This is how our custom ingestion scripts ( src/ingestion/*.py ) are invoked. BashOperator : Used to execute shell commands. This is the primary way dbt commands ( dbt run , dbt test , dbt seed , dbt deps ) are executed. PostgresOperator (Potentially): Could be used to execute specific SQL statements directly against PostgreSQL if needed, though most SQL transformations are handled by dbt. Tasks: An instantiated Operator within a DAG becomes a Task. Tasks have upstream and downstream dependencies, defining the order of execution. Scheduler: The Airflow component that monitors all DAGs and Triggers Task Instances whose dependencies have been met and schedule has been reached. Webserver (UI): Provides a user interface for monitoring DAG runs, task statuses, logs, and managing Airflow configurations. Accessible at http://localhost:8080 in our Dockerized setup. Connections & Variables (Managed by Airflow): While our current setup primarily uses environment variables (passed via Docker Compose) for database credentials needed by dbt and Python scripts, Airflow has its own system for managing connections (e.g., to PostgreSQL) and variables. For more complex setups, these could be utilized.","title":"Core Airflow Concepts Utilized"},{"location":"05_airflow_orchestration/#main-elt-dag-structure-crypto_data_ingestion_pipeline_vxpy","text":"The primary DAG typically has the following structure and flow: DAG Definition: dag_id , start_date , schedule_interval (e.g., '@daily' ), catchup=False (usually for production), default arguments (retries, email on failure, etc.). Task Definitions: Start Task (Optional DummyOperator ): Marks the beginning of the DAG. Ingestion Tasks (Parallel): A PythonOperator task for CoinGecko data ingestion (e.g., task_ingest_coingecko ). Calls a Python function like ingest_all_coingecko_data_callable() which in turn calls the core logic in src/ingestion/coingecko_ingestor.py . A PythonOperator task for Blockchair data ingestion (e.g., task_ingest_blockchair ). Calls a Python function like ingest_all_blockchair_data_callable() which then calls functions in src/ingestion/blockchair_ingestor.py for blocks and transactions. These two main ingestion tasks can often run in parallel as they deal with different data sources. dbt Pre-Transformation Tasks (Sequential): task_dbt_deps ( BashOperator ): Executes dbt deps --project-dir ... to install any dbt package dependencies. This should run before any other dbt command if packages are used. task_dbt_seed ( BashOperator ): Executes dbt seed --project-dir ... to load static data from CSV files in the seeds/ directory. dbt Transformation Task: task_dbt_run ( BashOperator ): Executes dbt run --project-dir ... to materialize all dbt models (staging, intermediate, marts). This is the core transformation step. dbt Testing Task: task_dbt_test ( BashOperator ): Executes dbt test --project-dir ... to run all defined data quality tests on the transformed models. End Task (Optional DummyOperator ): Marks the successful completion of the DAG. Task Dependencies (Defining the Flow): Using >> (set downstream) and << (set upstream) operators or lists for parallel execution. # Example Dependency Flow start_task = DummyOperator ( task_id = 'start' , dag = dag ) end_task = DummyOperator ( task_id = 'end' , dag = dag ) # Ingestion tasks can run in parallel ingestion_tasks = [ task_ingest_coingecko , task_ingest_blockchair ] start_task >> ingestion_tasks # Both ingestion tasks depend on start # dbt tasks run sequentially after ingestion # Ensure all ingestion tasks are complete before starting dbt sequence for task in ingestion_tasks : task >> task_dbt_deps task_dbt_deps >> task_dbt_seed >> task_dbt_run >> task_dbt_test >> end_task","title":"Main ELT DAG Structure (crypto_data_ingestion_pipeline_vX.py)"},{"location":"05_airflow_orchestration/#configuration-and-environment","text":"PYTHONPATH: The Airflow workers/scheduler need access to the src/ directory to import Python ingestion modules. The Docker setup mounts src/ and the DAG file often includes logic to add the project root to sys.path . DBT_PROJECT_DIR & DBT_PROFILES_DIR: The BashOperator tasks executing dbt commands need to specify the --project-dir to point to the mounted dbt_project/ directory. DBT_PROFILES_DIR (or the default ~/.dbt/ ) must contain a profiles.yml configured to connect to the postgres_app_db service, using environment variables for credentials. Environment Variables for dbt: The BashOperator tasks for dbt pass necessary database connection environment variables ( DB_HOST , DB_USER_DBT , DB_PASSWORD_DBT , etc.) which are then used by the profiles.yml env_var() function. These are sourced from the .env file by Docker Compose.","title":"Configuration and Environment"},{"location":"05_airflow_orchestration/#logging-and-monitoring","text":"Task Logs: All stdout and stderr from Python scripts and dbt commands are captured by Airflow and accessible per task instance in the Airflow UI. This is crucial for debugging. DAG Runs View: Shows the status of all DAG runs (success, running, failed). Graph View / Gantt Chart: Visualize task dependencies and execution times. Alerting (Optional): Airflow can be configured to send email alerts on task failures or SLA misses (though not implemented in the basic setup).","title":"Logging and Monitoring"},{"location":"05_airflow_orchestration/#idempotency","text":"DAGs: Airflow DAGs themselves are generally idempotent in terms of definition. Tasks: Ingestion scripts aim for idempotency (e.g., ON CONFLICT DO NOTHING or ON CONFLICT DO UPDATE ). dbt run is generally idempotent for view and table materializations (it recreates or replaces them). For incremental models, dbt handles idempotency based on the incremental strategy and unique keys. dbt seed can be made idempotent using flags like --full-refresh or by designing seeds to handle existing data. By leveraging Airflow, the project achieves a robust, automated, and monitorable ELT pipeline.","title":"Idempotency"},{"location":"06_powerbi_integration/","text":"Power BI Integration Guide \u00b6 This document provides guidance on connecting Microsoft Power BI Desktop to the PostgreSQL data warehouse populated by this ELT pipeline, and some best practices for building effective reports and dashboards. Connecting Power BI to PostgreSQL \u00b6 The transformed data resides in the postgres_app_db service, specifically within the analytical schemas created by dbt (e.g., analytics or marts ). Steps to Connect: Ensure Services are Running: Verify that your Docker Compose services are up and running, especially the postgres_app_db container. You can check with docker ps . Open Power BI Desktop. Get Data: On the \"Home\" ribbon, click \"Get Data\". Select \"Database\" from the categories on the left. Choose \"PostgreSQL database\" from the list and click \"Connect\". PostgreSQL Database Connection Details: Server: localhost Explanation: The docker-compose.yaml maps port 5432 of the postgres_app_db container to port 5432 on your host machine ( localhost ). Database: Enter the name of your application database. This is defined by the DB_NAME_APP variable in your .env file (e.g., crypto_raw_db ). Data Connectivity mode: Import (Recommended for most scenarios): Power BI imports a copy of the data into its internal VertiPaq engine. This generally provides the best performance for report interaction and allows for complex DAX calculations. Suitable for datasets that fit in memory. DirectQuery: Power BI sends queries directly to the PostgreSQL database each time a visual is interacted with. Use this if your dataset is extremely large and cannot fit in memory, or if you need near real-time data (though our pipeline is batch-oriented). DirectQuery can be slower and has limitations on DAX functions. (Advanced options like SQL statement or command timeout are usually not needed for initial connection.) Credentials: When prompted for credentials, select the \"Database\" tab on the left. User name: Enter the database user defined by DB_USER_APP in your .env file. Password: Enter the password defined by DB_PASSWORD_APP in your .env file. Encryption: You can choose the level of encryption if your PostgreSQL server is configured for SSL (our basic Docker setup is not, but a production setup would be). For local development, \"None\" might be acceptable if connecting to localhost . Click \"Connect\". Navigator Window: Once connected, the Navigator window will appear, showing all schemas and tables available in the specified database. Expand the schema where your dbt marts reside (e.g., analytics or marts ). Select the fact tables (e.g., fct_transactions , fct_daily_market_summary ) and dimension tables (e.g., dim_date , dim_cryptocurrency , dim_block ) that you need for your analysis. You can preview the data for each selected table. Click \"Load\" (to import data directly) or \"Transform Data\" (to open the Power Query Editor for further shaping before loading). It's generally good practice to at least briefly review in Power Query Editor. Best Practices in Power BI \u00b6 Use the Marts Layer: Always connect to the tables in your dbt marts (or analytics ) schema. These tables are cleaned, modeled, and optimized for reporting. Avoid connecting directly to raw or staging tables. Data Modeling in Power BI (if needed): Relationships: Power BI will attempt to auto-detect relationships based on column names. Verify these and create them manually if needed. Ensure relationships are correctly defined between fact tables and dimension tables using their respective primary and foreign keys (e.g., dim_date.date_key to fct_transactions.transaction_date_key ). Cardinality: Set the correct cardinality (one-to-many, many-to-one) and cross-filter direction for your relationships. Hide Foreign Keys: In the \"Model\" view, hide foreign key columns in fact tables from the \"Report\" view to simplify the field list for end-users. They should use attributes from dimension tables for filtering and grouping. DAX for Measures: Create explicit measures using DAX (Data Analysis Expressions) for all calculations and aggregations (e.g., Total Transaction Value = SUM(fct_transactions[output_total_usd]) ). Avoid using implicit measures (dragging numeric fields directly into visuals and relying on Power BI's default aggregation). Explicit measures offer more control and reusability. Organize measures into dedicated tables or using display folders. Dimension Tables for Slicers and Filters: Use columns from your dimension tables ( dim_date , dim_cryptocurrency , etc.) for slicers, filters, and as axes in your visuals. Optimize Performance: Reduce Cardinality: High cardinality columns (many unique values) can impact performance. Calculated Columns vs. Measures: Prefer measures over calculated columns where possible, especially for aggregations. Calculated columns are computed row-by-row during data refresh and consume memory. Filter Early: Apply filters in the Power Query Editor or at the report/page/visual level to reduce the amount of data processed. Use STAR SCHEMA principles: This is inherently what dbt helps create. Report Design: Keep reports clean and uncluttered. Use appropriate visuals for the type of data and analysis. Provide clear titles and labels. Consider user experience and navigation. Example Analysis Scenarios \u00b6 Once connected, you can build visuals for: Transaction Volume Over Time: Line chart with dim_date.full_date on the axis and a measure for COUNTROWS(fct_transactions) or SUM(fct_transactions[output_total_usd]) as values, sliced by dim_cryptocurrency.symbol . Average Transaction Fee by Cryptocurrency: Bar chart with dim_cryptocurrency.symbol on the axis and a measure for AVERAGE(fct_transactions[fee_usd]) as values. Price Trends: Line chart with dim_date.full_date on the axis and AVERAGE(fct_daily_market_summary[price_usd]) as values, sliced by dim_cryptocurrency.symbol . Number of Blocks Mined Per Day: Using dim_block joined with dim_date . By following these guidelines, you can effectively leverage the data prepared by the ELT pipeline to gain valuable insights into cryptocurrency activity and market trends using Power BI.","title":"Power BI Integration"},{"location":"06_powerbi_integration/#power-bi-integration-guide","text":"This document provides guidance on connecting Microsoft Power BI Desktop to the PostgreSQL data warehouse populated by this ELT pipeline, and some best practices for building effective reports and dashboards.","title":"Power BI Integration Guide"},{"location":"06_powerbi_integration/#connecting-power-bi-to-postgresql","text":"The transformed data resides in the postgres_app_db service, specifically within the analytical schemas created by dbt (e.g., analytics or marts ). Steps to Connect: Ensure Services are Running: Verify that your Docker Compose services are up and running, especially the postgres_app_db container. You can check with docker ps . Open Power BI Desktop. Get Data: On the \"Home\" ribbon, click \"Get Data\". Select \"Database\" from the categories on the left. Choose \"PostgreSQL database\" from the list and click \"Connect\". PostgreSQL Database Connection Details: Server: localhost Explanation: The docker-compose.yaml maps port 5432 of the postgres_app_db container to port 5432 on your host machine ( localhost ). Database: Enter the name of your application database. This is defined by the DB_NAME_APP variable in your .env file (e.g., crypto_raw_db ). Data Connectivity mode: Import (Recommended for most scenarios): Power BI imports a copy of the data into its internal VertiPaq engine. This generally provides the best performance for report interaction and allows for complex DAX calculations. Suitable for datasets that fit in memory. DirectQuery: Power BI sends queries directly to the PostgreSQL database each time a visual is interacted with. Use this if your dataset is extremely large and cannot fit in memory, or if you need near real-time data (though our pipeline is batch-oriented). DirectQuery can be slower and has limitations on DAX functions. (Advanced options like SQL statement or command timeout are usually not needed for initial connection.) Credentials: When prompted for credentials, select the \"Database\" tab on the left. User name: Enter the database user defined by DB_USER_APP in your .env file. Password: Enter the password defined by DB_PASSWORD_APP in your .env file. Encryption: You can choose the level of encryption if your PostgreSQL server is configured for SSL (our basic Docker setup is not, but a production setup would be). For local development, \"None\" might be acceptable if connecting to localhost . Click \"Connect\". Navigator Window: Once connected, the Navigator window will appear, showing all schemas and tables available in the specified database. Expand the schema where your dbt marts reside (e.g., analytics or marts ). Select the fact tables (e.g., fct_transactions , fct_daily_market_summary ) and dimension tables (e.g., dim_date , dim_cryptocurrency , dim_block ) that you need for your analysis. You can preview the data for each selected table. Click \"Load\" (to import data directly) or \"Transform Data\" (to open the Power Query Editor for further shaping before loading). It's generally good practice to at least briefly review in Power Query Editor.","title":"Connecting Power BI to PostgreSQL"},{"location":"06_powerbi_integration/#best-practices-in-power-bi","text":"Use the Marts Layer: Always connect to the tables in your dbt marts (or analytics ) schema. These tables are cleaned, modeled, and optimized for reporting. Avoid connecting directly to raw or staging tables. Data Modeling in Power BI (if needed): Relationships: Power BI will attempt to auto-detect relationships based on column names. Verify these and create them manually if needed. Ensure relationships are correctly defined between fact tables and dimension tables using their respective primary and foreign keys (e.g., dim_date.date_key to fct_transactions.transaction_date_key ). Cardinality: Set the correct cardinality (one-to-many, many-to-one) and cross-filter direction for your relationships. Hide Foreign Keys: In the \"Model\" view, hide foreign key columns in fact tables from the \"Report\" view to simplify the field list for end-users. They should use attributes from dimension tables for filtering and grouping. DAX for Measures: Create explicit measures using DAX (Data Analysis Expressions) for all calculations and aggregations (e.g., Total Transaction Value = SUM(fct_transactions[output_total_usd]) ). Avoid using implicit measures (dragging numeric fields directly into visuals and relying on Power BI's default aggregation). Explicit measures offer more control and reusability. Organize measures into dedicated tables or using display folders. Dimension Tables for Slicers and Filters: Use columns from your dimension tables ( dim_date , dim_cryptocurrency , etc.) for slicers, filters, and as axes in your visuals. Optimize Performance: Reduce Cardinality: High cardinality columns (many unique values) can impact performance. Calculated Columns vs. Measures: Prefer measures over calculated columns where possible, especially for aggregations. Calculated columns are computed row-by-row during data refresh and consume memory. Filter Early: Apply filters in the Power Query Editor or at the report/page/visual level to reduce the amount of data processed. Use STAR SCHEMA principles: This is inherently what dbt helps create. Report Design: Keep reports clean and uncluttered. Use appropriate visuals for the type of data and analysis. Provide clear titles and labels. Consider user experience and navigation.","title":"Best Practices in Power BI"},{"location":"06_powerbi_integration/#example-analysis-scenarios","text":"Once connected, you can build visuals for: Transaction Volume Over Time: Line chart with dim_date.full_date on the axis and a measure for COUNTROWS(fct_transactions) or SUM(fct_transactions[output_total_usd]) as values, sliced by dim_cryptocurrency.symbol . Average Transaction Fee by Cryptocurrency: Bar chart with dim_cryptocurrency.symbol on the axis and a measure for AVERAGE(fct_transactions[fee_usd]) as values. Price Trends: Line chart with dim_date.full_date on the axis and AVERAGE(fct_daily_market_summary[price_usd]) as values, sliced by dim_cryptocurrency.symbol . Number of Blocks Mined Per Day: Using dim_block joined with dim_date . By following these guidelines, you can effectively leverage the data prepared by the ELT pipeline to gain valuable insights into cryptocurrency activity and market trends using Power BI.","title":"Example Analysis Scenarios"},{"location":"07_development_guide/","text":"Development Guide \u00b6 This guide provides instructions and best practices for developers contributing to the Crypto Analytics BI project. Local Development Setup \u00b6 Refer to the main README.md for detailed instructions on setting up the Dockerized environment using docker-compose . Key aspects include: Cloning the repository. Creating and configuring the .env file. Ensuring Docker Desktop (or Docker Engine + Compose) is running. Building the Docker images ( docker-compose build ). Starting the services ( docker-compose up -d ). Initializing Airflow (database and user creation) on the first run. Working with Apache Airflow DAGs \u00b6 Location: DAG files are located in the ./dags/ directory, which is mounted into the Airflow scheduler and webserver containers. Creating/Modifying DAGs: Edit Python files in the ./dags/ directory on your host machine. Airflow's scheduler will automatically pick up changes to DAG files (usually within a few minutes, configurable in airflow.cfg ). New DAGs will appear in the Airflow UI. Testing DAGs: Airflow UI: Trigger DAGs manually, clear task states, and inspect logs. Airflow CLI: You can exec into the airflow-scheduler or airflow-webserver container and use the Airflow CLI for more advanced testing (e.g., airflow dags test <dag_id> <execution_date> ). PYTHONPATH: The src/ directory is mounted into Airflow containers and added to PYTHONPATH (often done in the DAG file itself or via Dockerfile configuration) so that DAGs can import custom Python modules from src.ingestion or src.common . Working with Python Ingestion Scripts \u00b6 Location: Ingestion logic resides in ./src/ingestion/ . Common utilities are in ./src/common/ . Development: You can develop and test these scripts locally on your host machine if you have Python and the necessary libraries ( requests , psycopg2-binary ) installed, and can connect to the Dockerized PostgreSQL ( postgres_app_db on localhost:5432 ). Alternatively, make changes and test by triggering the corresponding Airflow tasks, then inspecting logs via the Airflow UI. Dependencies: Python dependencies for Airflow (and thus for the environment where ingestion scripts run within Docker) are managed in docker/airflow/requirements_airflow.txt . If you add new dependencies for your ingestion scripts, update this file and rebuild the Airflow Docker image ( docker-compose build airflow-scheduler ). Working with dbt \u00b6 Location: The dbt project is in ./dbt_project/ . This directory is mounted into the Airflow containers. dbt profiles.yml : For Airflow to run dbt commands, it needs a profiles.yml that can connect to the postgres_app_db service. The recommended approach is to use environment variables within profiles.yml (e.g., host: \"{{ env_var('DB_HOST') }}\" ). These environment variables ( DB_HOST , DB_USER_DBT , etc.) are passed from the project's root .env file to the Airflow containers via docker-compose.yaml . If running dbt commands locally on your host machine (outside Docker, but connecting to the Dockerized DB), your local ~/.dbt/profiles.yml should point to host: localhost , port: 5432 , and use the DB_USER_APP , DB_PASSWORD_APP , DB_NAME_APP credentials from the root .env file. Running dbt Commands: Via Airflow: Airflow DAGs use BashOperator to execute dbt commands. This is the standard way transformations are run in the orchestrated pipeline. ```bash # Example command in Airflow DAG's BashOperator dbt run --project-dir /opt/airflow/dbt_project_for_airflow --profiles-dir /opt/airflow/dbt_project_for_airflow # (if profiles.yml is also mounted there, or relies purely on env_vars) ``` Manually within Docker: 1. docker exec -it <airflow_container_name_or_id> bash 2. cd /opt/airflow/dbt_project_for_airflow 3. dbt run --select my_model Locally on Host (if dbt installed): 1. Ensure your local ~/.dbt/profiles.yml is configured for localhost:5432 . 2. cd dbt_project/ 3. dbt run --select my_model Developing dbt Models: Edit SQL files in ./dbt_project/models/ . Follow dbt best practices: use staging layers, create modular models, write tests, and document your models in .yml files. Run dbt compile to see the compiled SQL. Run dbt run and dbt test frequently during development. dbt Dependencies: If you add packages to dbt_project/packages.yml , run dbt deps (locally or via an Airflow task) to install them. The target/ and dbt_packages/ directories are gitignored. Coding Standards and Conventions \u00b6 Python: Follow PEP 8 guidelines. Use a linter like Flake8 or Black for code formatting. SQL (dbt): Use consistent formatting (e.g., SQLFluff can be integrated with dbt). Use CTEs for readability in complex models. Clearly name models, columns, and CTEs. Comment your code where necessary. Airflow DAGs: Use clear task names. Define explicit dependencies. Keep DAG definitions concise; move complex Python logic into separate modules in src/ . Commit Messages: Follow conventional commit message formats (e.g., feat: add new transaction fact model , fix: correct coingecko api date formatting ). Branching Strategy (Example) \u00b6 main (or master ): Represents the stable, production-ready state. develop : Integration branch for features. Feature branches: feature/<ticket_id>-<short-description> (e.g., feature/CRYPTO-123-add-doge-market-data ). Bugfix branches: fix/<ticket_id>-<short-description> . Pull Requests (PRs) should be used to merge feature/fix branches into develop , and develop into main . PRs should be reviewed. Environment Variables \u00b6 All sensitive credentials (API keys, database passwords) and environment-specific configurations should be managed via the root .env file and accessed as environment variables within the Docker containers. DO NOT commit the .env file to Git. A .env.example file should be provided. Updating Dependencies \u00b6 Python (Airflow/Ingestion): Update versions in docker/airflow/requirements_airflow.txt , then rebuild the Docker image: docker-compose build airflow-scheduler (or relevant service). dbt Packages: Update versions in dbt_project/packages.yml , then run dbt deps (either locally, within the container, or via an Airflow task). This guide provides a starting point for development. Adapt and expand it as your team and project grow.","title":"Development Guide"},{"location":"07_development_guide/#development-guide","text":"This guide provides instructions and best practices for developers contributing to the Crypto Analytics BI project.","title":"Development Guide"},{"location":"07_development_guide/#local-development-setup","text":"Refer to the main README.md for detailed instructions on setting up the Dockerized environment using docker-compose . Key aspects include: Cloning the repository. Creating and configuring the .env file. Ensuring Docker Desktop (or Docker Engine + Compose) is running. Building the Docker images ( docker-compose build ). Starting the services ( docker-compose up -d ). Initializing Airflow (database and user creation) on the first run.","title":"Local Development Setup"},{"location":"07_development_guide/#working-with-apache-airflow-dags","text":"Location: DAG files are located in the ./dags/ directory, which is mounted into the Airflow scheduler and webserver containers. Creating/Modifying DAGs: Edit Python files in the ./dags/ directory on your host machine. Airflow's scheduler will automatically pick up changes to DAG files (usually within a few minutes, configurable in airflow.cfg ). New DAGs will appear in the Airflow UI. Testing DAGs: Airflow UI: Trigger DAGs manually, clear task states, and inspect logs. Airflow CLI: You can exec into the airflow-scheduler or airflow-webserver container and use the Airflow CLI for more advanced testing (e.g., airflow dags test <dag_id> <execution_date> ). PYTHONPATH: The src/ directory is mounted into Airflow containers and added to PYTHONPATH (often done in the DAG file itself or via Dockerfile configuration) so that DAGs can import custom Python modules from src.ingestion or src.common .","title":"Working with Apache Airflow DAGs"},{"location":"07_development_guide/#working-with-python-ingestion-scripts","text":"Location: Ingestion logic resides in ./src/ingestion/ . Common utilities are in ./src/common/ . Development: You can develop and test these scripts locally on your host machine if you have Python and the necessary libraries ( requests , psycopg2-binary ) installed, and can connect to the Dockerized PostgreSQL ( postgres_app_db on localhost:5432 ). Alternatively, make changes and test by triggering the corresponding Airflow tasks, then inspecting logs via the Airflow UI. Dependencies: Python dependencies for Airflow (and thus for the environment where ingestion scripts run within Docker) are managed in docker/airflow/requirements_airflow.txt . If you add new dependencies for your ingestion scripts, update this file and rebuild the Airflow Docker image ( docker-compose build airflow-scheduler ).","title":"Working with Python Ingestion Scripts"},{"location":"07_development_guide/#working-with-dbt","text":"Location: The dbt project is in ./dbt_project/ . This directory is mounted into the Airflow containers. dbt profiles.yml : For Airflow to run dbt commands, it needs a profiles.yml that can connect to the postgres_app_db service. The recommended approach is to use environment variables within profiles.yml (e.g., host: \"{{ env_var('DB_HOST') }}\" ). These environment variables ( DB_HOST , DB_USER_DBT , etc.) are passed from the project's root .env file to the Airflow containers via docker-compose.yaml . If running dbt commands locally on your host machine (outside Docker, but connecting to the Dockerized DB), your local ~/.dbt/profiles.yml should point to host: localhost , port: 5432 , and use the DB_USER_APP , DB_PASSWORD_APP , DB_NAME_APP credentials from the root .env file. Running dbt Commands: Via Airflow: Airflow DAGs use BashOperator to execute dbt commands. This is the standard way transformations are run in the orchestrated pipeline. ```bash # Example command in Airflow DAG's BashOperator dbt run --project-dir /opt/airflow/dbt_project_for_airflow --profiles-dir /opt/airflow/dbt_project_for_airflow # (if profiles.yml is also mounted there, or relies purely on env_vars) ``` Manually within Docker: 1. docker exec -it <airflow_container_name_or_id> bash 2. cd /opt/airflow/dbt_project_for_airflow 3. dbt run --select my_model Locally on Host (if dbt installed): 1. Ensure your local ~/.dbt/profiles.yml is configured for localhost:5432 . 2. cd dbt_project/ 3. dbt run --select my_model Developing dbt Models: Edit SQL files in ./dbt_project/models/ . Follow dbt best practices: use staging layers, create modular models, write tests, and document your models in .yml files. Run dbt compile to see the compiled SQL. Run dbt run and dbt test frequently during development. dbt Dependencies: If you add packages to dbt_project/packages.yml , run dbt deps (locally or via an Airflow task) to install them. The target/ and dbt_packages/ directories are gitignored.","title":"Working with dbt"},{"location":"07_development_guide/#coding-standards-and-conventions","text":"Python: Follow PEP 8 guidelines. Use a linter like Flake8 or Black for code formatting. SQL (dbt): Use consistent formatting (e.g., SQLFluff can be integrated with dbt). Use CTEs for readability in complex models. Clearly name models, columns, and CTEs. Comment your code where necessary. Airflow DAGs: Use clear task names. Define explicit dependencies. Keep DAG definitions concise; move complex Python logic into separate modules in src/ . Commit Messages: Follow conventional commit message formats (e.g., feat: add new transaction fact model , fix: correct coingecko api date formatting ).","title":"Coding Standards and Conventions"},{"location":"07_development_guide/#branching-strategy-example","text":"main (or master ): Represents the stable, production-ready state. develop : Integration branch for features. Feature branches: feature/<ticket_id>-<short-description> (e.g., feature/CRYPTO-123-add-doge-market-data ). Bugfix branches: fix/<ticket_id>-<short-description> . Pull Requests (PRs) should be used to merge feature/fix branches into develop , and develop into main . PRs should be reviewed.","title":"Branching Strategy (Example)"},{"location":"07_development_guide/#environment-variables","text":"All sensitive credentials (API keys, database passwords) and environment-specific configurations should be managed via the root .env file and accessed as environment variables within the Docker containers. DO NOT commit the .env file to Git. A .env.example file should be provided.","title":"Environment Variables"},{"location":"07_development_guide/#updating-dependencies","text":"Python (Airflow/Ingestion): Update versions in docker/airflow/requirements_airflow.txt , then rebuild the Docker image: docker-compose build airflow-scheduler (or relevant service). dbt Packages: Update versions in dbt_project/packages.yml , then run dbt deps (either locally, within the container, or via an Airflow task). This guide provides a starting point for development. Adapt and expand it as your team and project grow.","title":"Updating Dependencies"},{"location":"08_troubleshooting/","text":"Troubleshooting Guide \u00b6 This guide provides solutions and debugging tips for common issues encountered while working with the Crypto Analytics BI project. Docker Compose Issues \u00b6 Services Fail to Start / docker-compose up Errors: Check Docker Desktop/Engine: Ensure Docker is running and has sufficient resources (CPU, memory, disk space) allocated. Port Conflicts: If a port defined in docker-compose.yaml (e.g., 8080 for Airflow, 5432 for PostgreSQL) is already in use on your host machine, the service will fail to start. Solution: Stop the conflicting service on your host or change the host-side port mapping in docker-compose.yaml (e.g., ports: - \"8081:8080\" ). Volume Mount Issues: Ensure the local directories you are mounting as volumes (e.g., ./dags , ./dbt_project ) exist and have correct permissions. On Windows, file path issues with Docker Desktop can sometimes occur. Ensure paths are correctly specified. Insufficient Disk Space: Docker images, volumes, and logs can consume significant disk space. Clean up unused Docker resources: docker system prune -a --volumes . .env File Missing or Incorrect: Ensure the .env file exists in the project root and contains all necessary variables with correct values. Inspect Logs: Use docker-compose logs <service_name> (e.g., docker-compose logs airflow-scheduler ) to see detailed error messages from a specific service. Permission Denied Errors for Mounted Volumes (Especially for Airflow Logs/DAGs): This often happens because the user inside the Docker container (e.g., airflow user with UID 50000) doesn't have write permissions to the directory mounted from the host. Solution 1 (Recommended in docker-compose.yaml ): Set the user: \"${AIRFLOW_UID:-50000}:0\" (or your host user's UID/GID) for Airflow services in docker-compose.yaml . You might need to create an AIRFLOW_UID variable in your .env file with your host user's UID ( id -u on Linux/macOS). Solution 2 (Host-side): Change permissions on the host directories (e.g., chmod -R 777 ./logs ./dags ), but this is less secure. Apache Airflow Issues \u00b6 DAGs Not Appearing in UI / Not Scheduling: Syntax Errors in DAG File: Check Airflow scheduler logs ( docker-compose logs airflow-scheduler ) for Python import errors or syntax errors in your DAG file. The UI might also show a \"Broken DAG\" error. dags_folder Configuration: Ensure the dags_folder in airflow.cfg (or the volume mount in docker-compose.yaml ) correctly points to your dags/ directory. Scheduler Not Running: Verify the airflow-scheduler container is running ( docker ps ). start_date in the Future: If a DAG's start_date is in the future, it won't be scheduled until that date is reached (unless catchup=True and the schedule has passed). schedule_interval is None or @once : Such DAGs only run when manually triggered (or once if catchup=True for a past start_date ). Tasks Failing: Inspect Task Logs: The most crucial step. In the Airflow UI, go to the DAG run, click on the failed task instance, and then \"Log\". This will show stdout and stderr from the task's execution. PYTHONPATH Issues: If PythonOperator tasks fail with ModuleNotFoundError , ensure the src/ directory (or other custom module locations) is correctly added to PYTHONPATH for the Airflow workers/scheduler. This is often handled by mounting src/ and adding to sys.path in the DAG file or via Dockerfile. Environment Variables Missing: If scripts rely on environment variables (e.g., API keys, database credentials), ensure they are correctly passed to the Airflow execution environment (via .env and docker-compose.yaml ). Database Connection Errors (from Python scripts): Verify credentials, host ( postgres_app_db ), port ( 5432 ), and database name. Ensure the postgres_app_db service is healthy. Rate Limits: If ingesting from APIs, tasks might fail due to hitting rate limits. Implement delays or more robust retry mechanisms. Airflow Webserver/Scheduler Fails to Connect to Metadata Database ( postgres_airflow_meta ): Check logs of airflow-webserver and airflow-scheduler . Verify the SQL_ALCHEMY_CONN string in the Airflow environment configuration (set in docker-compose.yaml ) is correct and that postgres_airflow_meta service is healthy and its credentials match. dbt Issues \u00b6 dbt run or dbt test Fails (when run by Airflow or manually): Connection Errors (Profile Issues): Ensure profiles.yml is correctly configured to connect to postgres_app_db (host should be the service name postgres_app_db when running inside Docker, or localhost if running dbt locally and connecting to the exposed Docker port). Verify database credentials ( DB_USER_DBT , DB_PASSWORD_DBT , DB_NAME etc.) are correct and available as environment variables if profiles.yml uses env_var() . Test the connection using dbt debug --project-dir ... --profiles-dir ... . SQL Errors in Models: The dbt output will usually show the failing SQL model and the database error message. Copy the compiled SQL (from target/compiled/ ) and run it directly against PostgreSQL using a SQL client for easier debugging. Model Not Found ( {{ ref('model_name') }} or {{ source(...) }} errors): Check for typos in model names or source definitions. Ensure the referenced model/source exists and dbt can find it (check paths in dbt_project.yml ). Run dbt compile to see if dbt can resolve references. Permission Issues in Database: The dbt database user ( DB_USER_DBT ) must have appropriate permissions (SELECT on source tables/schemas, CREATE/INSERT/SELECT/DROP on target schemas like staging , analytics ). Test Failures ( dbt test ): dbt output will indicate which tests failed and often provide sample failing rows. Investigate the data in the relevant tables to understand why the assertion failed. dbt deps Fails: Check packages.yml for correct package names and versions. Ensure you have internet connectivity if fetching packages from dbt Hub. PostgreSQL Issues \u00b6 Cannot Connect to Database (from host SQL client or Power BI): Ensure the postgres_app_db container is running ( docker ps ). Verify the port mapping in docker-compose.yaml (e.g., 5432:5432 ). You should connect to localhost:5432 from your host. Check credentials ( DB_USER_APP , DB_PASSWORD_APP , DB_NAME_APP from .env ). Check PostgreSQL logs within the container: docker logs <postgres_app_db_container_name> . Permission Denied in PostgreSQL: The user connecting to the database (e.g., DB_USER_APP for Power BI, DB_USER_DBT for dbt) needs appropriate privileges on the schemas and tables they need to access. You might need to GRANT USAGE ON SCHEMA <schema_name> TO <user_name>; and GRANT SELECT ON ALL TABLES IN SCHEMA <schema_name> TO <user_name>; etc. General Debugging Steps \u00b6 Isolate the Problem: Try to determine which component is failing (Docker, Airflow, dbt, PostgreSQL, Python script). Check Logs: This is almost always the first and most important step. docker-compose logs <service_name> Airflow Task Logs (via UI) dbt command line output (and logs/dbt.log in your dbt_project or the target directory if configured) Simplify: If a complex DAG or dbt model fails, try running a simpler version or individual parts to pinpoint the error. Consult Documentation: Refer to the official documentation for Airflow, dbt, PostgreSQL, and the APIs you are using. Google the Error Message: Often, someone else has encountered a similar issue. Remember to be specific when asking for help (e.g., on Stack Overflow or team channels), providing: * The exact error message. * Relevant code snippets. * Steps to reproduce the issue. * Your environment details (OS, Docker version, tool versions).","title":"Troubleshooting"},{"location":"08_troubleshooting/#troubleshooting-guide","text":"This guide provides solutions and debugging tips for common issues encountered while working with the Crypto Analytics BI project.","title":"Troubleshooting Guide"},{"location":"08_troubleshooting/#docker-compose-issues","text":"Services Fail to Start / docker-compose up Errors: Check Docker Desktop/Engine: Ensure Docker is running and has sufficient resources (CPU, memory, disk space) allocated. Port Conflicts: If a port defined in docker-compose.yaml (e.g., 8080 for Airflow, 5432 for PostgreSQL) is already in use on your host machine, the service will fail to start. Solution: Stop the conflicting service on your host or change the host-side port mapping in docker-compose.yaml (e.g., ports: - \"8081:8080\" ). Volume Mount Issues: Ensure the local directories you are mounting as volumes (e.g., ./dags , ./dbt_project ) exist and have correct permissions. On Windows, file path issues with Docker Desktop can sometimes occur. Ensure paths are correctly specified. Insufficient Disk Space: Docker images, volumes, and logs can consume significant disk space. Clean up unused Docker resources: docker system prune -a --volumes . .env File Missing or Incorrect: Ensure the .env file exists in the project root and contains all necessary variables with correct values. Inspect Logs: Use docker-compose logs <service_name> (e.g., docker-compose logs airflow-scheduler ) to see detailed error messages from a specific service. Permission Denied Errors for Mounted Volumes (Especially for Airflow Logs/DAGs): This often happens because the user inside the Docker container (e.g., airflow user with UID 50000) doesn't have write permissions to the directory mounted from the host. Solution 1 (Recommended in docker-compose.yaml ): Set the user: \"${AIRFLOW_UID:-50000}:0\" (or your host user's UID/GID) for Airflow services in docker-compose.yaml . You might need to create an AIRFLOW_UID variable in your .env file with your host user's UID ( id -u on Linux/macOS). Solution 2 (Host-side): Change permissions on the host directories (e.g., chmod -R 777 ./logs ./dags ), but this is less secure.","title":"Docker Compose Issues"},{"location":"08_troubleshooting/#apache-airflow-issues","text":"DAGs Not Appearing in UI / Not Scheduling: Syntax Errors in DAG File: Check Airflow scheduler logs ( docker-compose logs airflow-scheduler ) for Python import errors or syntax errors in your DAG file. The UI might also show a \"Broken DAG\" error. dags_folder Configuration: Ensure the dags_folder in airflow.cfg (or the volume mount in docker-compose.yaml ) correctly points to your dags/ directory. Scheduler Not Running: Verify the airflow-scheduler container is running ( docker ps ). start_date in the Future: If a DAG's start_date is in the future, it won't be scheduled until that date is reached (unless catchup=True and the schedule has passed). schedule_interval is None or @once : Such DAGs only run when manually triggered (or once if catchup=True for a past start_date ). Tasks Failing: Inspect Task Logs: The most crucial step. In the Airflow UI, go to the DAG run, click on the failed task instance, and then \"Log\". This will show stdout and stderr from the task's execution. PYTHONPATH Issues: If PythonOperator tasks fail with ModuleNotFoundError , ensure the src/ directory (or other custom module locations) is correctly added to PYTHONPATH for the Airflow workers/scheduler. This is often handled by mounting src/ and adding to sys.path in the DAG file or via Dockerfile. Environment Variables Missing: If scripts rely on environment variables (e.g., API keys, database credentials), ensure they are correctly passed to the Airflow execution environment (via .env and docker-compose.yaml ). Database Connection Errors (from Python scripts): Verify credentials, host ( postgres_app_db ), port ( 5432 ), and database name. Ensure the postgres_app_db service is healthy. Rate Limits: If ingesting from APIs, tasks might fail due to hitting rate limits. Implement delays or more robust retry mechanisms. Airflow Webserver/Scheduler Fails to Connect to Metadata Database ( postgres_airflow_meta ): Check logs of airflow-webserver and airflow-scheduler . Verify the SQL_ALCHEMY_CONN string in the Airflow environment configuration (set in docker-compose.yaml ) is correct and that postgres_airflow_meta service is healthy and its credentials match.","title":"Apache Airflow Issues"},{"location":"08_troubleshooting/#dbt-issues","text":"dbt run or dbt test Fails (when run by Airflow or manually): Connection Errors (Profile Issues): Ensure profiles.yml is correctly configured to connect to postgres_app_db (host should be the service name postgres_app_db when running inside Docker, or localhost if running dbt locally and connecting to the exposed Docker port). Verify database credentials ( DB_USER_DBT , DB_PASSWORD_DBT , DB_NAME etc.) are correct and available as environment variables if profiles.yml uses env_var() . Test the connection using dbt debug --project-dir ... --profiles-dir ... . SQL Errors in Models: The dbt output will usually show the failing SQL model and the database error message. Copy the compiled SQL (from target/compiled/ ) and run it directly against PostgreSQL using a SQL client for easier debugging. Model Not Found ( {{ ref('model_name') }} or {{ source(...) }} errors): Check for typos in model names or source definitions. Ensure the referenced model/source exists and dbt can find it (check paths in dbt_project.yml ). Run dbt compile to see if dbt can resolve references. Permission Issues in Database: The dbt database user ( DB_USER_DBT ) must have appropriate permissions (SELECT on source tables/schemas, CREATE/INSERT/SELECT/DROP on target schemas like staging , analytics ). Test Failures ( dbt test ): dbt output will indicate which tests failed and often provide sample failing rows. Investigate the data in the relevant tables to understand why the assertion failed. dbt deps Fails: Check packages.yml for correct package names and versions. Ensure you have internet connectivity if fetching packages from dbt Hub.","title":"dbt Issues"},{"location":"08_troubleshooting/#postgresql-issues","text":"Cannot Connect to Database (from host SQL client or Power BI): Ensure the postgres_app_db container is running ( docker ps ). Verify the port mapping in docker-compose.yaml (e.g., 5432:5432 ). You should connect to localhost:5432 from your host. Check credentials ( DB_USER_APP , DB_PASSWORD_APP , DB_NAME_APP from .env ). Check PostgreSQL logs within the container: docker logs <postgres_app_db_container_name> . Permission Denied in PostgreSQL: The user connecting to the database (e.g., DB_USER_APP for Power BI, DB_USER_DBT for dbt) needs appropriate privileges on the schemas and tables they need to access. You might need to GRANT USAGE ON SCHEMA <schema_name> TO <user_name>; and GRANT SELECT ON ALL TABLES IN SCHEMA <schema_name> TO <user_name>; etc.","title":"PostgreSQL Issues"},{"location":"08_troubleshooting/#general-debugging-steps","text":"Isolate the Problem: Try to determine which component is failing (Docker, Airflow, dbt, PostgreSQL, Python script). Check Logs: This is almost always the first and most important step. docker-compose logs <service_name> Airflow Task Logs (via UI) dbt command line output (and logs/dbt.log in your dbt_project or the target directory if configured) Simplify: If a complex DAG or dbt model fails, try running a simpler version or individual parts to pinpoint the error. Consult Documentation: Refer to the official documentation for Airflow, dbt, PostgreSQL, and the APIs you are using. Google the Error Message: Often, someone else has encountered a similar issue. Remember to be specific when asking for help (e.g., on Stack Overflow or team channels), providing: * The exact error message. * Relevant code snippets. * Steps to reproduce the issue. * Your environment details (OS, Docker version, tool versions).","title":"General Debugging Steps"},{"location":"04_dbt_data_modeling/","text":"dbt Data Modeling \u00b6 Data modeling in this project is handled by dbt (Data Build Tool) . dbt allows us to transform the raw data ingested into PostgreSQL into a clean, reliable, and analytics-ready dimensional model. All dbt files reside in the dbt_project/ directory. Core Principles of dbt Usage Here \u00b6 ELT (Extract, Load, Transform): We first load raw data into PostgreSQL, then use dbt to perform transformations within the database. Modularity: Transformations are broken down into small, understandable SQL models. Version Control: All dbt models (SQL files) are version controlled with Git, treating analytics code like software. Testing: Data quality and integrity are enforced through dbt tests. Documentation: dbt helps generate documentation for the data models. Directory Structure and Key Files \u00b6 Refer to dbt_project/README.md or the main project README.md for a detailed breakdown of the dbt_project/ directory structure. Key components include: dbt_project.yml : Main project configuration. profiles.yml : Database connection details (managed outside Git for security). models/ : Contains all SQL transformation logic. sources.yml : Defines and documents raw source tables. staging/ : Basic cleaning, renaming, and type casting of source data. intermediate/ (Optional): For complex, multi-step transformations. marts/ : Final, user-facing fact and dimension tables. seeds/ : For static lookup data. tests/ : Custom data quality tests (schema tests are often defined in model .yml files). macros/ : Reusable SQL snippets. Data Modeling Layers \u00b6 The transformation process is typically structured in layers: Sources (Defined in models/**/sources.yml ): These aren't models themselves but declarations that point dbt to the raw tables created by the ingestion pipeline (e.g., raw_blockchain_blocks , raw_market_prices_volumes in the public schema). Allows for basic testing (e.g., freshness, row counts) and documentation at the source level. Referenced in staging models using {{ source('source_name', 'table_name') }} . See: Sources Documentation Staging Layer ( models/staging/ ): Purpose: To perform light transformations on each source table. This layer acts as a \"cleaned\" version of the raw data. Transformations: Renaming columns to consistent, business-friendly names (e.g., block_time to block_timestamp_utc ). Casting data types to appropriate SQL types (e.g., string timestamps to TIMESTAMP ). Basic calculations (e.g., extracting date parts from timestamps). Filtering out test records or unnecessary data (if applicable). Materialization: Often materialized as view s for faster development iterations and to avoid data duplication, but can be table s if performance is critical for downstream models. Naming Convention: stg_<source_system>__<object_name> (e.g., stg_blockchair__blocks , stg_coingecko__prices_volumes ). Referenced by intermediate or mart models using {{ ref('staging_model_name') }} . See: Staging Layer Documentation Intermediate Layer ( models/intermediate/ - Optional): Purpose: To break down complex transformations that might involve joining multiple staging models or performing multi-step aggregations before building the final facts or dimensions. Use Cases: Pre-aggregating data, complex joins, preparing data for multiple downstream marts. Materialization: Can be ephemeral (CTE within downstream models), view , or table . Naming Convention: int_<description_of_transformation> . Marts Layer ( models/marts/ ): Purpose: To create the final, analytics-ready tables that Power BI users will query. This layer typically implements a dimensional model (star or snowflake schema). Dimension Tables ( dim_ ): Describe the \"who, what, where, when, why\" of the data. Examples: dim_date , dim_cryptocurrency , dim_block . Contain descriptive attributes and usually have a surrogate primary key. Fact Tables ( fct_ ): Contain measurable facts (metrics) and foreign keys that link to dimension tables. Examples: fct_transactions , fct_daily_market_summary . Granularity is key (e.g., one row per transaction, one row per coin per day). Materialization: Almost always materialized as table s for optimal query performance by BI tools. See: Marts Layer Documentation and Dimensional Model Documentation Dimensional Model \u00b6 The target data model for the marts layer is a dimensional model (likely a star schema for simplicity or a snowflake schema if dimensions become complex and normalized). This structure is optimized for analytical queries and BI tools. Facts: Quantitative measures (e.g., transaction value, number of blocks, trading volume). Dimensions: Qualitative context for the facts (e.g., date, cryptocurrency type, block details). See Dimensional Model Documentation for more details. Testing and Data Quality \u00b6 dbt provides a robust framework for testing data: Schema Tests: Defined in .yml files alongside model definitions. Common tests include: unique : Ensures values in a column are unique. not_null : Ensures a column does not contain null values. accepted_values : Ensures column values are within a predefined set. relationships : Tests referential integrity between tables (e.g., a foreign key in a fact table must exist in the corresponding dimension table). Custom Data Tests (Singular Tests): SQL queries written in the tests/ directory that should return zero rows if the test passes. Used for more complex business logic validation. dbt test command: Executes all defined tests. Airflow DAGs should include a step to run dbt test after dbt run . See Testing and Quality Documentation . Orchestration with Airflow \u00b6 Airflow DAGs trigger dbt commands in the correct sequence: dbt deps (if packages.yml is used, to install dependencies) dbt seed (to load static data) dbt run (to materialize all models) dbt test (to validate data quality) This ensures that transformations are executed only after new raw data is available and that data quality is checked before it's consumed by Power BI.","title":"dbt Data Modeling"},{"location":"04_dbt_data_modeling/#dbt-data-modeling","text":"Data modeling in this project is handled by dbt (Data Build Tool) . dbt allows us to transform the raw data ingested into PostgreSQL into a clean, reliable, and analytics-ready dimensional model. All dbt files reside in the dbt_project/ directory.","title":"dbt Data Modeling"},{"location":"04_dbt_data_modeling/#core-principles-of-dbt-usage-here","text":"ELT (Extract, Load, Transform): We first load raw data into PostgreSQL, then use dbt to perform transformations within the database. Modularity: Transformations are broken down into small, understandable SQL models. Version Control: All dbt models (SQL files) are version controlled with Git, treating analytics code like software. Testing: Data quality and integrity are enforced through dbt tests. Documentation: dbt helps generate documentation for the data models.","title":"Core Principles of dbt Usage Here"},{"location":"04_dbt_data_modeling/#directory-structure-and-key-files","text":"Refer to dbt_project/README.md or the main project README.md for a detailed breakdown of the dbt_project/ directory structure. Key components include: dbt_project.yml : Main project configuration. profiles.yml : Database connection details (managed outside Git for security). models/ : Contains all SQL transformation logic. sources.yml : Defines and documents raw source tables. staging/ : Basic cleaning, renaming, and type casting of source data. intermediate/ (Optional): For complex, multi-step transformations. marts/ : Final, user-facing fact and dimension tables. seeds/ : For static lookup data. tests/ : Custom data quality tests (schema tests are often defined in model .yml files). macros/ : Reusable SQL snippets.","title":"Directory Structure and Key Files"},{"location":"04_dbt_data_modeling/#data-modeling-layers","text":"The transformation process is typically structured in layers: Sources (Defined in models/**/sources.yml ): These aren't models themselves but declarations that point dbt to the raw tables created by the ingestion pipeline (e.g., raw_blockchain_blocks , raw_market_prices_volumes in the public schema). Allows for basic testing (e.g., freshness, row counts) and documentation at the source level. Referenced in staging models using {{ source('source_name', 'table_name') }} . See: Sources Documentation Staging Layer ( models/staging/ ): Purpose: To perform light transformations on each source table. This layer acts as a \"cleaned\" version of the raw data. Transformations: Renaming columns to consistent, business-friendly names (e.g., block_time to block_timestamp_utc ). Casting data types to appropriate SQL types (e.g., string timestamps to TIMESTAMP ). Basic calculations (e.g., extracting date parts from timestamps). Filtering out test records or unnecessary data (if applicable). Materialization: Often materialized as view s for faster development iterations and to avoid data duplication, but can be table s if performance is critical for downstream models. Naming Convention: stg_<source_system>__<object_name> (e.g., stg_blockchair__blocks , stg_coingecko__prices_volumes ). Referenced by intermediate or mart models using {{ ref('staging_model_name') }} . See: Staging Layer Documentation Intermediate Layer ( models/intermediate/ - Optional): Purpose: To break down complex transformations that might involve joining multiple staging models or performing multi-step aggregations before building the final facts or dimensions. Use Cases: Pre-aggregating data, complex joins, preparing data for multiple downstream marts. Materialization: Can be ephemeral (CTE within downstream models), view , or table . Naming Convention: int_<description_of_transformation> . Marts Layer ( models/marts/ ): Purpose: To create the final, analytics-ready tables that Power BI users will query. This layer typically implements a dimensional model (star or snowflake schema). Dimension Tables ( dim_ ): Describe the \"who, what, where, when, why\" of the data. Examples: dim_date , dim_cryptocurrency , dim_block . Contain descriptive attributes and usually have a surrogate primary key. Fact Tables ( fct_ ): Contain measurable facts (metrics) and foreign keys that link to dimension tables. Examples: fct_transactions , fct_daily_market_summary . Granularity is key (e.g., one row per transaction, one row per coin per day). Materialization: Almost always materialized as table s for optimal query performance by BI tools. See: Marts Layer Documentation and Dimensional Model Documentation","title":"Data Modeling Layers"},{"location":"04_dbt_data_modeling/#dimensional-model","text":"The target data model for the marts layer is a dimensional model (likely a star schema for simplicity or a snowflake schema if dimensions become complex and normalized). This structure is optimized for analytical queries and BI tools. Facts: Quantitative measures (e.g., transaction value, number of blocks, trading volume). Dimensions: Qualitative context for the facts (e.g., date, cryptocurrency type, block details). See Dimensional Model Documentation for more details.","title":"Dimensional Model"},{"location":"04_dbt_data_modeling/#testing-and-data-quality","text":"dbt provides a robust framework for testing data: Schema Tests: Defined in .yml files alongside model definitions. Common tests include: unique : Ensures values in a column are unique. not_null : Ensures a column does not contain null values. accepted_values : Ensures column values are within a predefined set. relationships : Tests referential integrity between tables (e.g., a foreign key in a fact table must exist in the corresponding dimension table). Custom Data Tests (Singular Tests): SQL queries written in the tests/ directory that should return zero rows if the test passes. Used for more complex business logic validation. dbt test command: Executes all defined tests. Airflow DAGs should include a step to run dbt test after dbt run . See Testing and Quality Documentation .","title":"Testing and Data Quality"},{"location":"04_dbt_data_modeling/#orchestration-with-airflow","text":"Airflow DAGs trigger dbt commands in the correct sequence: dbt deps (if packages.yml is used, to install dependencies) dbt seed (to load static data) dbt run (to materialize all models) dbt test (to validate data quality) This ensures that transformations are executed only after new raw data is available and that data quality is checked before it's consumed by Power BI.","title":"Orchestration with Airflow"},{"location":"04_dbt_data_modeling/04_01_dimensional_model/","text":"Dimensional Model Overview \u00b6 The core analytical data model implemented in the marts layer of our dbt project is a dimensional model . This approach, popularized by Ralph Kimball, structures data for optimal query performance, understandability, and ease of use with BI tools like Power BI. Key Concepts \u00b6 Facts: Represent business measurements or metrics. These are typically numeric and additive. Stored in Fact Tables . Examples: Transaction value, number of transactions, daily trading volume, number of blocks mined. Dimensions: Provide the context for the facts. They describe the \"who, what, where, when, why, and how\" related to a business process event. Stored in Dimension Tables . Examples: Date, Cryptocurrency, Block details, Address (if modeled). Dimension tables contain descriptive attributes (e.g., month name, cryptocurrency full name, block hash). Star Schema (Primary Goal): The most common dimensional model structure. Characterized by one or more fact tables referencing any number of dimension tables. Visually resembles a star, with the fact table at the center and dimension tables radiating outwards. Generally denormalized for simpler joins and faster queries. Snowflake Schema (Potential Evolution): An extension of a star schema where dimension tables are normalized into multiple related tables. Can reduce data redundancy in large dimensions but may lead to more complex queries with more joins. We aim for a star schema initially but may snowflake dimensions if they become very large and have highly redundant attributes. Proposed Core Dimensions \u00b6 dim_date : Grain: One row per day. Attributes: date_key (surrogate key), full_date , year , month , month_name , day_of_month , day_of_week_name , quarter , is_weekend , etc. Purpose: Allows slicing and dicing facts by various time attributes. Essential for time-series analysis. dim_cryptocurrency : Grain: One row per cryptocurrency being analyzed (BTC, ETH, DOGE). Attributes: crypto_key (surrogate key), symbol (e.g., BTC), full_name (e.g., Bitcoin), coin_gecko_id , blockchair_coin_name , relevant project details. Source: Primarily from seeds/cryptocurrency_metadata.csv and potentially enriched from API responses. Purpose: Allows filtering and grouping facts by cryptocurrency. dim_block (Blockchain Specific): Grain: One row per unique block across all analyzed blockchains. Attributes: block_pk (surrogate key), block_id (native ID), block_hash , coin_symbol (FK to dim_cryptocurrency ), block_timestamp_utc , block_date_key (FK to dim_date ), transaction_count_in_block , block_size_bytes , difficulty . Purpose: Provides context for blockchain-related facts, such as transactions. Proposed Core Fact Tables \u00b6 fct_transactions : Grain: One row per blockchain transaction. Foreign Keys: transaction_date_key (to dim_date ), crypto_key (to dim_cryptocurrency ), block_pk (to dim_block ). Degenerate Dimensions: tx_hash (transaction hash itself, acting as a descriptive attribute). Measures (Facts): fee_usd , output_total_usd , input_count , output_count , tx_size_bytes , is_coinbase_flag (0 or 1). Purpose: Central table for analyzing transaction activity, fees, and values. fct_daily_market_summary : Grain: One row per cryptocurrency per day. Foreign Keys: price_date_key (to dim_date ), crypto_key (to dim_cryptocurrency ). Measures (Facts): price_usd , volume_usd , market_cap_usd . Purpose: Tracks daily market performance indicators for each cryptocurrency. fct_daily_block_summary (Potential): Grain: One row per cryptocurrency per day. Foreign Keys: summary_date_key (to dim_date ), crypto_key (to dim_cryptocurrency ). Measures (Facts): total_blocks_mined_on_day , total_transactions_in_blocks_on_day , average_difficulty_on_day , total_fees_collected_on_day_usd . Purpose: Provides aggregated daily on-chain activity metrics. This might be derived by aggregating dim_block or fct_transactions . Benefits of this Model \u00b6 Understandability: Easier for business users and analysts to comprehend the data relationships. Query Performance: Optimized for common analytical queries (filtering, grouping, aggregation). Joins are typically between a fact table and a few dimension tables. Ease of Use with BI Tools: Power BI and similar tools are designed to work effectively with star/snowflake schemas. Extensibility: Relatively straightforward to add new facts or dimensions as business requirements evolve. Implementation in dbt \u00b6 Dimension tables are created as dbt models in dbt_project/models/marts/core/dim_*.sql . Fact tables are created as dbt models in dbt_project/models/marts/core/fct_*.sql . Surrogate keys for dimensions can be generated using dbt_utils.generate_surrogate_key or by using database sequences if preferred for dim_ tables. Fact table primary keys can also be surrogate keys based on the combination of their degenerate dimensions and FKs. Relationships between fact and dimension tables are enforced and tested using dbt's relationships test. ```","title":"Dimensional Model"},{"location":"04_dbt_data_modeling/04_01_dimensional_model/#dimensional-model-overview","text":"The core analytical data model implemented in the marts layer of our dbt project is a dimensional model . This approach, popularized by Ralph Kimball, structures data for optimal query performance, understandability, and ease of use with BI tools like Power BI.","title":"Dimensional Model Overview"},{"location":"04_dbt_data_modeling/04_01_dimensional_model/#key-concepts","text":"Facts: Represent business measurements or metrics. These are typically numeric and additive. Stored in Fact Tables . Examples: Transaction value, number of transactions, daily trading volume, number of blocks mined. Dimensions: Provide the context for the facts. They describe the \"who, what, where, when, why, and how\" related to a business process event. Stored in Dimension Tables . Examples: Date, Cryptocurrency, Block details, Address (if modeled). Dimension tables contain descriptive attributes (e.g., month name, cryptocurrency full name, block hash). Star Schema (Primary Goal): The most common dimensional model structure. Characterized by one or more fact tables referencing any number of dimension tables. Visually resembles a star, with the fact table at the center and dimension tables radiating outwards. Generally denormalized for simpler joins and faster queries. Snowflake Schema (Potential Evolution): An extension of a star schema where dimension tables are normalized into multiple related tables. Can reduce data redundancy in large dimensions but may lead to more complex queries with more joins. We aim for a star schema initially but may snowflake dimensions if they become very large and have highly redundant attributes.","title":"Key Concepts"},{"location":"04_dbt_data_modeling/04_01_dimensional_model/#proposed-core-dimensions","text":"dim_date : Grain: One row per day. Attributes: date_key (surrogate key), full_date , year , month , month_name , day_of_month , day_of_week_name , quarter , is_weekend , etc. Purpose: Allows slicing and dicing facts by various time attributes. Essential for time-series analysis. dim_cryptocurrency : Grain: One row per cryptocurrency being analyzed (BTC, ETH, DOGE). Attributes: crypto_key (surrogate key), symbol (e.g., BTC), full_name (e.g., Bitcoin), coin_gecko_id , blockchair_coin_name , relevant project details. Source: Primarily from seeds/cryptocurrency_metadata.csv and potentially enriched from API responses. Purpose: Allows filtering and grouping facts by cryptocurrency. dim_block (Blockchain Specific): Grain: One row per unique block across all analyzed blockchains. Attributes: block_pk (surrogate key), block_id (native ID), block_hash , coin_symbol (FK to dim_cryptocurrency ), block_timestamp_utc , block_date_key (FK to dim_date ), transaction_count_in_block , block_size_bytes , difficulty . Purpose: Provides context for blockchain-related facts, such as transactions.","title":"Proposed Core Dimensions"},{"location":"04_dbt_data_modeling/04_01_dimensional_model/#proposed-core-fact-tables","text":"fct_transactions : Grain: One row per blockchain transaction. Foreign Keys: transaction_date_key (to dim_date ), crypto_key (to dim_cryptocurrency ), block_pk (to dim_block ). Degenerate Dimensions: tx_hash (transaction hash itself, acting as a descriptive attribute). Measures (Facts): fee_usd , output_total_usd , input_count , output_count , tx_size_bytes , is_coinbase_flag (0 or 1). Purpose: Central table for analyzing transaction activity, fees, and values. fct_daily_market_summary : Grain: One row per cryptocurrency per day. Foreign Keys: price_date_key (to dim_date ), crypto_key (to dim_cryptocurrency ). Measures (Facts): price_usd , volume_usd , market_cap_usd . Purpose: Tracks daily market performance indicators for each cryptocurrency. fct_daily_block_summary (Potential): Grain: One row per cryptocurrency per day. Foreign Keys: summary_date_key (to dim_date ), crypto_key (to dim_cryptocurrency ). Measures (Facts): total_blocks_mined_on_day , total_transactions_in_blocks_on_day , average_difficulty_on_day , total_fees_collected_on_day_usd . Purpose: Provides aggregated daily on-chain activity metrics. This might be derived by aggregating dim_block or fct_transactions .","title":"Proposed Core Fact Tables"},{"location":"04_dbt_data_modeling/04_01_dimensional_model/#benefits-of-this-model","text":"Understandability: Easier for business users and analysts to comprehend the data relationships. Query Performance: Optimized for common analytical queries (filtering, grouping, aggregation). Joins are typically between a fact table and a few dimension tables. Ease of Use with BI Tools: Power BI and similar tools are designed to work effectively with star/snowflake schemas. Extensibility: Relatively straightforward to add new facts or dimensions as business requirements evolve.","title":"Benefits of this Model"},{"location":"04_dbt_data_modeling/04_01_dimensional_model/#implementation-in-dbt","text":"Dimension tables are created as dbt models in dbt_project/models/marts/core/dim_*.sql . Fact tables are created as dbt models in dbt_project/models/marts/core/fct_*.sql . Surrogate keys for dimensions can be generated using dbt_utils.generate_surrogate_key or by using database sequences if preferred for dim_ tables. Fact table primary keys can also be surrogate keys based on the combination of their degenerate dimensions and FKs. Relationships between fact and dimension tables are enforced and tested using dbt's relationships test. ```","title":"Implementation in dbt"},{"location":"04_dbt_data_modeling/04_02_sources/","text":"dbt Sources \u00b6 In dbt, sources are a way to declare and document the raw data tables that your dbt project ingests from external systems. They are the starting point for your dbt transformations. In our project, these are the tables in PostgreSQL populated by the Python ingestion scripts. Purpose of Defining Sources \u00b6 Documentation: Clearly document where your raw data comes from, its schema, and expected columns. Data Lineage: dbt docs generate uses source definitions to build a lineage graph, showing how raw data flows into your final models. Testing at Source: You can apply basic tests directly to source tables, such as: Freshness checks: Ensure the data isn't stale. Row count checks: Monitor if data loading is happening as expected. Basic column-level tests ( not_null , unique on expected keys). Abstraction: Staging models can then reference these sources using the {{ source('source_name', 'table_name') }} macro, making it easy to change the underlying raw table name or schema in one place ( sources.yml ) if needed. Implementation \u00b6 Source definitions are typically located in .yml files within your dbt_project/models/ directory, often grouped logically (e.g., models/staging/schema.yml , or a dedicated models/sources.yml ). File Example: dbt_project/models/sources.yml (or could be models/staging/sources.yml ) version : 2 sources : - name : raw_blockchair # A logical grouping for Blockchair data description : \"Raw data ingested from the Blockchair API for various cryptocurrencies.\" # You can specify the database and schema if it's different from your target profile's default # database: your_raw_database schema : public # The schema where ingestion scripts load Blockchair data (e.g., 'public' or 'raw_data') # loader: \"Airflow Ingestion DAGs\" # Optional metadata # loaded_at_field: \"etl_loaded_at_timestamp\" # If you have a load timestamp in raw tables for freshness tables : - name : raw_blockchain_blocks description : \"Raw blocks data ingested from Blockchair API for BTC, ETH, DOGE.\" # freshness: # Example freshness check (requires loaded_at_field) # warn_after: {count: 24, period: hour} # error_after: {count: 48, period: hour} columns : - name : block_id description : \"The unique ID (height) of the block.\" tests : - not_null # - unique # Block ID is unique per coin, so a composite unique test might be better - name : coin_symbol description : \"Symbol of the cryptocurrency (BTC, ETH, DOGE).\" tests : - not_null - name : block_hash description : \"The unique hash of the block.\" tests : - unique # Block hash should be globally unique - not_null - name : block_time description : \"Timestamp when the block was mined.\" # Add all other relevant columns from your raw_blockchain_blocks table - name : raw_blockchain_transactions description : \"Raw transactions data from Blockchair API.\" columns : - name : tx_hash description : \"The unique hash of the transaction.\" tests : - unique # Transaction hash should be globally unique - not_null - name : coin_symbol description : \"Symbol of the cryptocurrency.\" tests : - not_null - name : block_id description : \"ID of the block containing this transaction.\" # Add all other relevant columns - name : raw_coingecko # A logical grouping for CoinGecko data description : \"Raw market data ingested from the CoinGecko API.\" schema : public # The schema where ingestion scripts load CoinGecko data tables : - name : raw_market_prices_volumes description : \"Raw daily prices, volumes, and market caps from CoinGecko API.\" # loaded_at_field: \"updated_at\" # If CoinGecko provides an update timestamp # freshness: # warn_after: {count: 2, period: day} # Expect daily updates columns : - name : coin_id description : \"CoinGecko's unique identifier for the cryptocurrency.\" tests : - not_null - name : price_date description : \"The date for which the market data is recorded.\" tests : - not_null - name : price_usd description : \"Price in USD.\" - name : volume_usd description : \"Trading volume in USD.\" - name : market_cap_usd description : \"Market capitalization in USD.\" # Add composite unique test for coin_id and price_date tests : - unique : column_names : [ coin_id , price_date ] Key Configuration Options in sources.yml \u00b6 name (for source block): A unique name for the source system (e.g., raw_blockchair ). schema : The schema in your database where these raw tables reside. database (Optional): If the source tables are in a different database than your dbt target database. tables : A list of tables belonging to this source. name (for table block): The actual name of the table in your database. description : Human-readable description of the table. columns : A list of columns in the table. name (for column block): The actual column name. description : Human-readable description of the column. tests : A list of schema tests to apply to this column (e.g., unique , not_null , accepted_values , relationships ). tests (at table level): A list of tests that apply to the whole table (e.g., a multi-column unique test). freshness (Optional): Defines acceptable data latency. Requires a timestamp column in the source table ( loaded_at_field ) indicating when the data was last updated. loaded_at_field (Optional): Specifies the column used for freshness checks. Referencing Sources in Models \u00b6 In your staging models (e.g., models/staging/stg_blockchair__blocks.sql ), you will refer to these source tables using the source() macro: SELECT * FROM {{ source ( 'raw_blockchair' , 'raw_blockchain_blocks' ) }} This decouples your staging logic from the exact physical location of the raw data, making your dbt project more maintainable. --- **`docs/04_dbt_data_modeling/04_03_staging_layer.md`** ```markdown # dbt Staging Layer The **staging layer** is the first transformation layer in our dbt project. Its primary purpose is to take the raw data ingested from external sources (as defined in `sources.yml`) and prepare it for more complex transformations and modeling in downstream layers (intermediate and marts). ## Goals of the Staging Layer 1. **One-to-One Mapping (Generally):** Typically, there is one staging model for each source table. 2. **Light Transformations:** Only basic, necessary transformations are performed here. Avoid complex business logic or joins between different source concepts in this layer. 3. **Data Cleansing:** * **Renaming Columns:** Rename columns to consistent, clear, and business-friendly names (e.g., `tx_time` to `transaction_timestamp_utc`). * **Type Casting:** Convert columns to their correct data types (e.g., string representations of dates to `DATE` or `TIMESTAMP`, string numbers to `NUMERIC` or `INTEGER`). * **Basic Formatting:** Apply minimal formatting if necessary (e.g., trimming whitespace, standardizing case for certain string fields \u2013 though this can also be done later). 4. **Deriving Basic Attributes:** Create simple derived columns if universally useful (e.g., extracting the date part from a timestamp: `DATE(transaction_timestamp_utc) AS transaction_date`). 5. **Filtering (Minimal):** Remove records that are unequivocally irrelevant for any downstream analysis (e.g., test records from a source system, completely malformed rows if not handled during ingestion). However, be cautious with filtering at this stage; it's often better to filter in downstream models where the business context is clearer. 6. **No Joins Between Different Source Entities:** Staging models should focus on a single source table. Joins to bring together different concepts (e.g., transactions with block details from a separate blocks table, if they were separate sources) happen in intermediate or mart layers. ## Implementation in dbt - Staging models are SQL files located in the `dbt_project/models/staging/` directory. - They typically use Common Table Expressions (CTEs) for readability, often starting with a CTE that selects from a `{{ source(...) }}`. - **Naming Convention:** `stg_<source_name>__<object_name>.sql` (e.g., `stg_blockchair__blocks.sql`, `stg_coingecko__prices_volumes.sql`). The double underscore `__` is a common convention to separate the source system from the object. ## Materialization - Staging models are often materialized as **`views`** in the database. - **Pros:** No additional storage used (queries are executed against underlying raw tables on-the-fly), always reflect the latest raw data, faster dbt runs during development as no data is physically moved. - **Cons:** Can be slower to query if the underlying raw tables are very large or if the view definition involves many calculations. - Alternatively, they can be materialized as **`tables`** if: - Downstream models frequently query them, and performance is critical. * You want to \"snapshot\" a version of the cleaned data. - The default materialization for the staging layer can be set in `dbt_project.yml`. **Example `dbt_project.yml` configuration for staging:** ```yaml models: crypto_analytics: # Your project name staging: +materialized: view # Default for all models in models/staging/ +schema: staging # Output schema in PostgreSQL Example Staging Model \u00b6 File: dbt_project/models/staging/blockchain/stg_blockchair__transactions.sql -- This CTE selects from the raw source table defined in sources.yml WITH source_data AS ( SELECT * FROM {{ source ( 'raw_blockchair' , 'raw_blockchain_transactions' ) }} ) -- This CTE performs the light transformations SELECT -- Identifiers & Keys tx_hash , -- Already a good name coin_symbol , -- Already a good name block_id , -- Already a good name -- Timestamps & Dates (Casting and Renaming) tx_time :: TIMESTAMP AS transaction_timestamp_utc , -- Casting to TIMESTAMP DATE ( tx_time ) AS transaction_date , -- Deriving date part -- Monetary Values (Casting to NUMERIC if they are strings in raw) fee_usd :: NUMERIC AS fee_usd , output_total_usd :: NUMERIC AS output_total_usd , -- Counts input_count :: INTEGER AS input_count , output_count :: INTEGER AS output_count , -- Other Attributes size_bytes :: INTEGER AS transaction_size_bytes , -- Renaming and casting is_coinbase :: BOOLEAN AS is_coinbase_transaction -- Casting if it's not already boolean -- Columns to exclude (if any) can be omitted from the SELECT list -- e.g., raw_api_response_column_we_dont_need FROM source_data -- Add WHERE clause for incremental processing if this model becomes incremental -- e.g., WHERE tx_time >= (SELECT MAX(transaction_timestamp_utc) FROM {{ this }}) -- For incremental tables Benefits of a Staging Layer \u00b6 DRY (Don't Repeat Yourself): Common cleaning logic is centralized, so you don't have to repeat it in multiple downstream models. Clarity and Readability: Downstream models ( intermediate and marts ) can then build upon this cleaned and consistently named data, making their logic simpler. Debugging: Easier to trace data issues back to their source. If a problem exists in a mart table, you can check the staging layer, and then the raw source. Improved Lineage: Provides a clear step in the data lineage graph. Performance (Potentially): If staging models are materialized as tables, they can pre-compute some transformations, speeding up downstream queries. The staging layer is a foundational part of a robust dbt project, ensuring that all subsequent transformations start from a common, clean baseline. ```","title":"Sources"},{"location":"04_dbt_data_modeling/04_02_sources/#dbt-sources","text":"In dbt, sources are a way to declare and document the raw data tables that your dbt project ingests from external systems. They are the starting point for your dbt transformations. In our project, these are the tables in PostgreSQL populated by the Python ingestion scripts.","title":"dbt Sources"},{"location":"04_dbt_data_modeling/04_02_sources/#purpose-of-defining-sources","text":"Documentation: Clearly document where your raw data comes from, its schema, and expected columns. Data Lineage: dbt docs generate uses source definitions to build a lineage graph, showing how raw data flows into your final models. Testing at Source: You can apply basic tests directly to source tables, such as: Freshness checks: Ensure the data isn't stale. Row count checks: Monitor if data loading is happening as expected. Basic column-level tests ( not_null , unique on expected keys). Abstraction: Staging models can then reference these sources using the {{ source('source_name', 'table_name') }} macro, making it easy to change the underlying raw table name or schema in one place ( sources.yml ) if needed.","title":"Purpose of Defining Sources"},{"location":"04_dbt_data_modeling/04_02_sources/#implementation","text":"Source definitions are typically located in .yml files within your dbt_project/models/ directory, often grouped logically (e.g., models/staging/schema.yml , or a dedicated models/sources.yml ). File Example: dbt_project/models/sources.yml (or could be models/staging/sources.yml ) version : 2 sources : - name : raw_blockchair # A logical grouping for Blockchair data description : \"Raw data ingested from the Blockchair API for various cryptocurrencies.\" # You can specify the database and schema if it's different from your target profile's default # database: your_raw_database schema : public # The schema where ingestion scripts load Blockchair data (e.g., 'public' or 'raw_data') # loader: \"Airflow Ingestion DAGs\" # Optional metadata # loaded_at_field: \"etl_loaded_at_timestamp\" # If you have a load timestamp in raw tables for freshness tables : - name : raw_blockchain_blocks description : \"Raw blocks data ingested from Blockchair API for BTC, ETH, DOGE.\" # freshness: # Example freshness check (requires loaded_at_field) # warn_after: {count: 24, period: hour} # error_after: {count: 48, period: hour} columns : - name : block_id description : \"The unique ID (height) of the block.\" tests : - not_null # - unique # Block ID is unique per coin, so a composite unique test might be better - name : coin_symbol description : \"Symbol of the cryptocurrency (BTC, ETH, DOGE).\" tests : - not_null - name : block_hash description : \"The unique hash of the block.\" tests : - unique # Block hash should be globally unique - not_null - name : block_time description : \"Timestamp when the block was mined.\" # Add all other relevant columns from your raw_blockchain_blocks table - name : raw_blockchain_transactions description : \"Raw transactions data from Blockchair API.\" columns : - name : tx_hash description : \"The unique hash of the transaction.\" tests : - unique # Transaction hash should be globally unique - not_null - name : coin_symbol description : \"Symbol of the cryptocurrency.\" tests : - not_null - name : block_id description : \"ID of the block containing this transaction.\" # Add all other relevant columns - name : raw_coingecko # A logical grouping for CoinGecko data description : \"Raw market data ingested from the CoinGecko API.\" schema : public # The schema where ingestion scripts load CoinGecko data tables : - name : raw_market_prices_volumes description : \"Raw daily prices, volumes, and market caps from CoinGecko API.\" # loaded_at_field: \"updated_at\" # If CoinGecko provides an update timestamp # freshness: # warn_after: {count: 2, period: day} # Expect daily updates columns : - name : coin_id description : \"CoinGecko's unique identifier for the cryptocurrency.\" tests : - not_null - name : price_date description : \"The date for which the market data is recorded.\" tests : - not_null - name : price_usd description : \"Price in USD.\" - name : volume_usd description : \"Trading volume in USD.\" - name : market_cap_usd description : \"Market capitalization in USD.\" # Add composite unique test for coin_id and price_date tests : - unique : column_names : [ coin_id , price_date ]","title":"Implementation"},{"location":"04_dbt_data_modeling/04_02_sources/#key-configuration-options-in-sourcesyml","text":"name (for source block): A unique name for the source system (e.g., raw_blockchair ). schema : The schema in your database where these raw tables reside. database (Optional): If the source tables are in a different database than your dbt target database. tables : A list of tables belonging to this source. name (for table block): The actual name of the table in your database. description : Human-readable description of the table. columns : A list of columns in the table. name (for column block): The actual column name. description : Human-readable description of the column. tests : A list of schema tests to apply to this column (e.g., unique , not_null , accepted_values , relationships ). tests (at table level): A list of tests that apply to the whole table (e.g., a multi-column unique test). freshness (Optional): Defines acceptable data latency. Requires a timestamp column in the source table ( loaded_at_field ) indicating when the data was last updated. loaded_at_field (Optional): Specifies the column used for freshness checks.","title":"Key Configuration Options in sources.yml"},{"location":"04_dbt_data_modeling/04_02_sources/#referencing-sources-in-models","text":"In your staging models (e.g., models/staging/stg_blockchair__blocks.sql ), you will refer to these source tables using the source() macro: SELECT * FROM {{ source ( 'raw_blockchair' , 'raw_blockchain_blocks' ) }} This decouples your staging logic from the exact physical location of the raw data, making your dbt project more maintainable. --- **`docs/04_dbt_data_modeling/04_03_staging_layer.md`** ```markdown # dbt Staging Layer The **staging layer** is the first transformation layer in our dbt project. Its primary purpose is to take the raw data ingested from external sources (as defined in `sources.yml`) and prepare it for more complex transformations and modeling in downstream layers (intermediate and marts). ## Goals of the Staging Layer 1. **One-to-One Mapping (Generally):** Typically, there is one staging model for each source table. 2. **Light Transformations:** Only basic, necessary transformations are performed here. Avoid complex business logic or joins between different source concepts in this layer. 3. **Data Cleansing:** * **Renaming Columns:** Rename columns to consistent, clear, and business-friendly names (e.g., `tx_time` to `transaction_timestamp_utc`). * **Type Casting:** Convert columns to their correct data types (e.g., string representations of dates to `DATE` or `TIMESTAMP`, string numbers to `NUMERIC` or `INTEGER`). * **Basic Formatting:** Apply minimal formatting if necessary (e.g., trimming whitespace, standardizing case for certain string fields \u2013 though this can also be done later). 4. **Deriving Basic Attributes:** Create simple derived columns if universally useful (e.g., extracting the date part from a timestamp: `DATE(transaction_timestamp_utc) AS transaction_date`). 5. **Filtering (Minimal):** Remove records that are unequivocally irrelevant for any downstream analysis (e.g., test records from a source system, completely malformed rows if not handled during ingestion). However, be cautious with filtering at this stage; it's often better to filter in downstream models where the business context is clearer. 6. **No Joins Between Different Source Entities:** Staging models should focus on a single source table. Joins to bring together different concepts (e.g., transactions with block details from a separate blocks table, if they were separate sources) happen in intermediate or mart layers. ## Implementation in dbt - Staging models are SQL files located in the `dbt_project/models/staging/` directory. - They typically use Common Table Expressions (CTEs) for readability, often starting with a CTE that selects from a `{{ source(...) }}`. - **Naming Convention:** `stg_<source_name>__<object_name>.sql` (e.g., `stg_blockchair__blocks.sql`, `stg_coingecko__prices_volumes.sql`). The double underscore `__` is a common convention to separate the source system from the object. ## Materialization - Staging models are often materialized as **`views`** in the database. - **Pros:** No additional storage used (queries are executed against underlying raw tables on-the-fly), always reflect the latest raw data, faster dbt runs during development as no data is physically moved. - **Cons:** Can be slower to query if the underlying raw tables are very large or if the view definition involves many calculations. - Alternatively, they can be materialized as **`tables`** if: - Downstream models frequently query them, and performance is critical. * You want to \"snapshot\" a version of the cleaned data. - The default materialization for the staging layer can be set in `dbt_project.yml`. **Example `dbt_project.yml` configuration for staging:** ```yaml models: crypto_analytics: # Your project name staging: +materialized: view # Default for all models in models/staging/ +schema: staging # Output schema in PostgreSQL","title":"Referencing Sources in Models"},{"location":"04_dbt_data_modeling/04_02_sources/#example-staging-model","text":"File: dbt_project/models/staging/blockchain/stg_blockchair__transactions.sql -- This CTE selects from the raw source table defined in sources.yml WITH source_data AS ( SELECT * FROM {{ source ( 'raw_blockchair' , 'raw_blockchain_transactions' ) }} ) -- This CTE performs the light transformations SELECT -- Identifiers & Keys tx_hash , -- Already a good name coin_symbol , -- Already a good name block_id , -- Already a good name -- Timestamps & Dates (Casting and Renaming) tx_time :: TIMESTAMP AS transaction_timestamp_utc , -- Casting to TIMESTAMP DATE ( tx_time ) AS transaction_date , -- Deriving date part -- Monetary Values (Casting to NUMERIC if they are strings in raw) fee_usd :: NUMERIC AS fee_usd , output_total_usd :: NUMERIC AS output_total_usd , -- Counts input_count :: INTEGER AS input_count , output_count :: INTEGER AS output_count , -- Other Attributes size_bytes :: INTEGER AS transaction_size_bytes , -- Renaming and casting is_coinbase :: BOOLEAN AS is_coinbase_transaction -- Casting if it's not already boolean -- Columns to exclude (if any) can be omitted from the SELECT list -- e.g., raw_api_response_column_we_dont_need FROM source_data -- Add WHERE clause for incremental processing if this model becomes incremental -- e.g., WHERE tx_time >= (SELECT MAX(transaction_timestamp_utc) FROM {{ this }}) -- For incremental tables","title":"Example Staging Model"},{"location":"04_dbt_data_modeling/04_02_sources/#benefits-of-a-staging-layer","text":"DRY (Don't Repeat Yourself): Common cleaning logic is centralized, so you don't have to repeat it in multiple downstream models. Clarity and Readability: Downstream models ( intermediate and marts ) can then build upon this cleaned and consistently named data, making their logic simpler. Debugging: Easier to trace data issues back to their source. If a problem exists in a mart table, you can check the staging layer, and then the raw source. Improved Lineage: Provides a clear step in the data lineage graph. Performance (Potentially): If staging models are materialized as tables, they can pre-compute some transformations, speeding up downstream queries. The staging layer is a foundational part of a robust dbt project, ensuring that all subsequent transformations start from a common, clean baseline. ```","title":"Benefits of a Staging Layer"},{"location":"04_dbt_data_modeling/04_04_marts_layer/","text":"dbt Marts Layer \u00b6 The marts layer (also known as the presentation or reporting layer) is the final transformation layer in our dbt project. Its purpose is to produce data models that are optimized for end-user consumption, typically by BI tools like Power BI, or for direct analytical querying. These models represent the \"single source of truth\" for specific business areas or analytical use cases. Goals of the Marts Layer \u00b6 Business-Centric View: Models are designed around business concepts and entities, making them intuitive for analysts and stakeholders. Performance Optimization: Data is structured (often in a dimensional model) to ensure fast query performance for common analytical workloads (slicing, dicing, aggregation). Dimensional Modeling: This layer typically implements dimensional models, most commonly star schemas or snowflake schemas. Fact Tables ( fct_ ): Contain quantitative measures (metrics) and foreign keys to dimension tables. They represent business processes or events. Dimension Tables ( dim_ ): Provide descriptive context to the facts (who, what, where, when, why). Aggregation (Optional but Common): Sometimes, aggregated mart tables are created to pre-compute common summaries, further improving query speed for specific dashboards or reports. Single Source of Truth: Marts provide a consistent and reliable dataset for specific analytical domains. Implementation in dbt \u00b6 Mart models are SQL files located in the dbt_project/models/marts/ directory. They are often further organized into subdirectories based on business domain (e.g., marts/core/ , marts/finance/ , marts/blockchain_activity/ ). They primarily SELECT from staging models ( {{ ref('stg_model_name') }} ) or intermediate models ( {{ ref('int_model_name') }} ). Joins are common here, especially between staging/intermediate models to construct facts and dimensions. Naming Convention: Dimension tables: dim_<entity_name>.sql (e.g., dim_date.sql , dim_cryptocurrency.sql ). Fact tables: fct_<business_process_or_event>.sql (e.g., fct_transactions.sql , fct_daily_market_summary.sql ). Materialization \u00b6 Mart models, especially fact tables and frequently used dimension tables, are almost always materialized as tables . Pros: Provides the best query performance for BI tools as the data is physically stored in the transformed structure. Cons: Requires more storage space and dbt run times can be longer as data is physically written. Some very small or rarely changing dimension tables might occasionally be views , but table is the norm for marts. Incremental Materialization: For large fact tables, an incremental materialization strategy ( materialized='incremental' ) is often used. This allows dbt to only process and insert/update new or changed data since the last run, significantly speeding up dbt runs. Example dbt_project.yml configuration for marts: models : crypto_analytics : # Your project name marts : +materialized : table # Default for all models in models/marts/ +schema : analytics # Output schema in PostgreSQL (or 'marts') # Example of configuring a sub-directory for incremental models # core: # fct_transactions: # specific model # +materialized: incremental # +unique_key: 'transaction_pk' # if using unique key for updates Example Mart Models \u00b6 Dimension Table Example \u00b6 File: dbt_project/models/marts/core/dim_cryptocurrency.sql {{ config ( materialized = 'table' ) }} -- Assuming you have a seed file for basic metadata WITH crypto_seed AS ( SELECT * FROM {{ ref ( 'cryptocurrency_metadata_seed' ) }} -- Assuming seed name ), -- You could potentially join with other sources if more attributes are available -- For example, if an API provided a 'first_transaction_date' per coin from a staging model -- enriched_coin_data AS ( ... ) SELECT -- Surrogate Key (if not using symbol directly as PK, which is also an option for small dims) {{ dbt_utils . generate_surrogate_key ([ 'cs.symbol' ]) }} AS crypto_key , cs . symbol , cs . full_name , cs . project_website , -- Add other relevant attributes from seeds or enriched sources LOWER ( cs . full_name ) AS blockchair_coin_name , -- Example derived attribute for Blockchair API if needed LOWER ( cs . full_name ) AS coingecko_coin_id -- Assuming full name matches CoinGecko ID for simplicity -- In reality, this would be a proper mapping FROM crypto_seed cs -- LEFT JOIN enriched_coin_data ecd ON cs.symbol = ecd.symbol (Note: cryptocurrency_metadata_seed would be a CSV file in your seeds/ directory loaded via dbt seed ) Fact Table Example \u00b6 File: dbt_project/models/marts/core/fct_daily_market_summary.sql {{ config ( materialized = 'table' // For incremental : // materialized = 'incremental' , // unique_key = [ 'price_date' , 'crypto_key' ] // Or a dedicated surrogate key // incremental_strategy = 'merge' // or 'delete+insert' ) }} WITH stg_prices AS ( SELECT * FROM {{ ref ( 'stg_coingecko__prices_volumes' ) }} ), dim_date AS ( SELECT full_date , {{ dbt_utils . generate_surrogate_key ([ 'full_date' ]) }} AS date_key -- Or your date SK logic FROM {{ ref ( 'dim_date' ) }} ), dim_crypto AS ( SELECT symbol , crypto_key , coingecko_coin_id FROM {{ ref ( 'dim_cryptocurrency' ) }} ) SELECT -- Surrogate key for the fact table {{ dbt_utils . generate_surrogate_key ([ 'sp.price_date' , 'dc.crypto_key' ]) }} AS daily_market_summary_pk , -- Foreign Keys dd . date_key AS price_date_key , dc . crypto_key , -- Degenerate Dimensions (natural keys for context) sp . price_date , dc . symbol AS crypto_symbol , -- Measures sp . price_usd , sp . volume_usd , sp . market_cap_usd FROM stg_prices sp JOIN dim_date dd ON sp . price_date = dd . full_date JOIN dim_crypto dc ON LOWER ( sp . coin_id ) = LOWER ( dc . coingecko_coin_id ) -- Robust join on CG ID { % if is_incremental () % } -- this filter will only be applied on an incremental run WHERE sp . price_date > ( SELECT MAX ( price_date ) FROM {{ this }} ) { % endif % } Relationship to BI Tools \u00b6 The tables created in the marts layer are the ones that Power BI (or any other BI tool) will connect to. The clear structure of facts and dimensions, well-defined relationships (enforced by dbt tests), and optimized performance make data analysis much more efficient and reliable. ``` docs/04_dbt_data_modeling/04_05_testing_and_quality.md ```markdown dbt Testing and Data Quality \u00b6 Ensuring data quality is paramount for any analytics project. dbt provides a powerful and convenient framework for defining, executing, and managing data tests directly within your data transformation workflow. This helps build trust in the data and allows for early detection of issues. Why Test Your Data? \u00b6 Accuracy: Verify that calculations are correct and data reflects reality. Completeness: Ensure no critical data is missing. Consistency: Check that data is consistent across different tables and models. Integrity: Maintain referential integrity between related entities (e.g., foreign keys). Reliability: Build confidence that the data powering dashboards and decisions is trustworthy. Early Detection: Catch data issues early in the pipeline, before they impact downstream users or reports. Types of Tests in dbt \u00b6 dbt supports several types of tests: Schema Tests (Generic Tests): These are predefined, out-of-the-box tests that can be applied to specific columns in your models. They are defined in .yml files, typically alongside your model definitions or in a dedicated schema.yml file within a model subdirectory. Common Schema Tests: unique : Asserts that all values in a column are unique. not_null : Asserts that there are no null values in a column. accepted_values : Asserts that all values in a column are within a specified list of allowed values. Example: tests: - accepted_values: values: ['BTC', 'ETH', 'DOGE'] relationships : Asserts referential integrity. It checks that every value in a column (foreign key) exists in a corresponding column of another model (primary key). Example: tests: - relationships: to: ref('dim_date') field: date_key Many more generic tests are available through packages like dbt-utils (e.g., equal_rowcount , expression_is_true ). Singular Data Tests (Custom Tests): These are custom SQL queries that you write to validate specific business logic or data conditions. They are stored as .sql files in the tests/ directory of your dbt project (often in tests/singular/ or tests/asserts/ ). A singular data test PASSES if the SQL query returns ZERO rows. If it returns one or more rows, the test FAILS, and those rows represent the records that violate the assertion. Example: A test to ensure that a transaction amount is always positive. sql -- tests/singular/assert_transaction_value_positive.sql SELECT transaction_pk, -- Or any relevant identifier transaction_amount_usd FROM {{ ref('fct_transactions') }} WHERE transaction_amount_usd < 0 Source Freshness (Specialized Test): Defined in sources.yml to monitor the timeliness of your raw data loads. Requires a loaded_at_field in your source table that indicates when the data was last updated. You can configure warn_after and error_after thresholds. Implementation Strategy \u00b6 Test at Every Layer: Sources: Basic not_null on key identifiers, unique where expected, and freshness checks. Staging Models: unique and not_null on primary keys/business keys, accepted_values for categorical columns. Mart Models (Dimensions): unique and not_null on surrogate keys and natural keys. Mart Models (Facts): not_null on foreign keys and key measures. relationships tests to ensure all foreign keys correctly point to existing dimension records. Custom business logic tests (e.g., sum of debits equals sum of credits). Define Tests in .yml Files: For schema tests, co-locate them with your model definitions in .yml files. This keeps the model's contract (columns, descriptions, tests) in one place. Example: models/marts/core/fct_transactions.yml ```yaml version: 2 models: - name: fct_transactions description: \"Fact table containing individual blockchain transactions with their associated measures and dimensional foreign keys.\" columns: - name: transaction_pk # Assuming a surrogate key for the fact table description: \"Surrogate primary key for the transaction fact.\" tests: - unique - not_null - name: tx_hash description: \"The original transaction hash.\" tests: - not_null - name: price_date_key description: \"Foreign key to dim_date, representing the date of the transaction.\" tests: - not_null - relationships: to: ref('dim_date') field: date_key # Assuming 'date_key' is the PK in dim_date - name: crypto_key description: \"Foreign key to dim_cryptocurrency.\" tests: - not_null - relationships: to: ref('dim_cryptocurrency') field: crypto_key - name: fee_usd description: \"Transaction fee in USD.\" tests: - dbt_utils.expression_is_true: # Using a test from dbt_utils package expression: \">= 0\" # Fee should not be negative # This could also be a singular test for more complex logic # ... other columns and their tests ``` Leverage dbt Packages: Install and use packages like dbt-utils which provide a rich set of additional generic tests (e.g., dbt_utils.not_accepted_values , dbt_utils.equal_rowcount , dbt_utils.expression_is_true ). Add to packages.yml : ```yaml packages: package: dbt-labs/dbt_utils version: [\">=1.1.0\", \"<1.2.0\"] # Specify version range ``` Run dbt deps to install. Write Custom SQL Tests for Business Logic: If a validation cannot be expressed with a generic schema test, write a singular data test in the tests/ directory. Running Tests \u00b6 Command: dbt test dbt test : Runs all tests (schema and singular). dbt test --select model_name : Runs tests defined on a specific model. dbt test --select source:source_name : Runs tests on a specific source. dbt test --select tag:my_tag : Runs tests associated with a specific tag. Output: dbt will report the status (PASS, FAIL, WARN, ERROR) of each test. For failing singular tests, it often shows the rows that failed the assertion. Integration with Airflow: Include a BashOperator task in your Airflow DAGs to execute dbt test after dbt run . The DAG can be configured to fail if any dbt tests fail, preventing bad data from being used downstream. ```python In Airflow DAG \u00b6 task_dbt_test = BashOperator( task_id='dbt_run_tests', bash_command=f\"dbt test --project-dir {DBT_PROJECT_DIR} --profiles-dir {DBT_PROFILES_DIR}\", # ... ) task_dbt_run >> task_dbt_test \u00b6 ``` By consistently applying tests throughout your dbt project, you significantly improve the reliability and trustworthiness of your data warehouse, providing a solid foundation for your BI and analytics efforts.","title":"Marts Layer"},{"location":"04_dbt_data_modeling/04_04_marts_layer/#dbt-marts-layer","text":"The marts layer (also known as the presentation or reporting layer) is the final transformation layer in our dbt project. Its purpose is to produce data models that are optimized for end-user consumption, typically by BI tools like Power BI, or for direct analytical querying. These models represent the \"single source of truth\" for specific business areas or analytical use cases.","title":"dbt Marts Layer"},{"location":"04_dbt_data_modeling/04_04_marts_layer/#goals-of-the-marts-layer","text":"Business-Centric View: Models are designed around business concepts and entities, making them intuitive for analysts and stakeholders. Performance Optimization: Data is structured (often in a dimensional model) to ensure fast query performance for common analytical workloads (slicing, dicing, aggregation). Dimensional Modeling: This layer typically implements dimensional models, most commonly star schemas or snowflake schemas. Fact Tables ( fct_ ): Contain quantitative measures (metrics) and foreign keys to dimension tables. They represent business processes or events. Dimension Tables ( dim_ ): Provide descriptive context to the facts (who, what, where, when, why). Aggregation (Optional but Common): Sometimes, aggregated mart tables are created to pre-compute common summaries, further improving query speed for specific dashboards or reports. Single Source of Truth: Marts provide a consistent and reliable dataset for specific analytical domains.","title":"Goals of the Marts Layer"},{"location":"04_dbt_data_modeling/04_04_marts_layer/#implementation-in-dbt","text":"Mart models are SQL files located in the dbt_project/models/marts/ directory. They are often further organized into subdirectories based on business domain (e.g., marts/core/ , marts/finance/ , marts/blockchain_activity/ ). They primarily SELECT from staging models ( {{ ref('stg_model_name') }} ) or intermediate models ( {{ ref('int_model_name') }} ). Joins are common here, especially between staging/intermediate models to construct facts and dimensions. Naming Convention: Dimension tables: dim_<entity_name>.sql (e.g., dim_date.sql , dim_cryptocurrency.sql ). Fact tables: fct_<business_process_or_event>.sql (e.g., fct_transactions.sql , fct_daily_market_summary.sql ).","title":"Implementation in dbt"},{"location":"04_dbt_data_modeling/04_04_marts_layer/#materialization","text":"Mart models, especially fact tables and frequently used dimension tables, are almost always materialized as tables . Pros: Provides the best query performance for BI tools as the data is physically stored in the transformed structure. Cons: Requires more storage space and dbt run times can be longer as data is physically written. Some very small or rarely changing dimension tables might occasionally be views , but table is the norm for marts. Incremental Materialization: For large fact tables, an incremental materialization strategy ( materialized='incremental' ) is often used. This allows dbt to only process and insert/update new or changed data since the last run, significantly speeding up dbt runs. Example dbt_project.yml configuration for marts: models : crypto_analytics : # Your project name marts : +materialized : table # Default for all models in models/marts/ +schema : analytics # Output schema in PostgreSQL (or 'marts') # Example of configuring a sub-directory for incremental models # core: # fct_transactions: # specific model # +materialized: incremental # +unique_key: 'transaction_pk' # if using unique key for updates","title":"Materialization"},{"location":"04_dbt_data_modeling/04_04_marts_layer/#example-mart-models","text":"","title":"Example Mart Models"},{"location":"04_dbt_data_modeling/04_04_marts_layer/#dimension-table-example","text":"File: dbt_project/models/marts/core/dim_cryptocurrency.sql {{ config ( materialized = 'table' ) }} -- Assuming you have a seed file for basic metadata WITH crypto_seed AS ( SELECT * FROM {{ ref ( 'cryptocurrency_metadata_seed' ) }} -- Assuming seed name ), -- You could potentially join with other sources if more attributes are available -- For example, if an API provided a 'first_transaction_date' per coin from a staging model -- enriched_coin_data AS ( ... ) SELECT -- Surrogate Key (if not using symbol directly as PK, which is also an option for small dims) {{ dbt_utils . generate_surrogate_key ([ 'cs.symbol' ]) }} AS crypto_key , cs . symbol , cs . full_name , cs . project_website , -- Add other relevant attributes from seeds or enriched sources LOWER ( cs . full_name ) AS blockchair_coin_name , -- Example derived attribute for Blockchair API if needed LOWER ( cs . full_name ) AS coingecko_coin_id -- Assuming full name matches CoinGecko ID for simplicity -- In reality, this would be a proper mapping FROM crypto_seed cs -- LEFT JOIN enriched_coin_data ecd ON cs.symbol = ecd.symbol (Note: cryptocurrency_metadata_seed would be a CSV file in your seeds/ directory loaded via dbt seed )","title":"Dimension Table Example"},{"location":"04_dbt_data_modeling/04_04_marts_layer/#fact-table-example","text":"File: dbt_project/models/marts/core/fct_daily_market_summary.sql {{ config ( materialized = 'table' // For incremental : // materialized = 'incremental' , // unique_key = [ 'price_date' , 'crypto_key' ] // Or a dedicated surrogate key // incremental_strategy = 'merge' // or 'delete+insert' ) }} WITH stg_prices AS ( SELECT * FROM {{ ref ( 'stg_coingecko__prices_volumes' ) }} ), dim_date AS ( SELECT full_date , {{ dbt_utils . generate_surrogate_key ([ 'full_date' ]) }} AS date_key -- Or your date SK logic FROM {{ ref ( 'dim_date' ) }} ), dim_crypto AS ( SELECT symbol , crypto_key , coingecko_coin_id FROM {{ ref ( 'dim_cryptocurrency' ) }} ) SELECT -- Surrogate key for the fact table {{ dbt_utils . generate_surrogate_key ([ 'sp.price_date' , 'dc.crypto_key' ]) }} AS daily_market_summary_pk , -- Foreign Keys dd . date_key AS price_date_key , dc . crypto_key , -- Degenerate Dimensions (natural keys for context) sp . price_date , dc . symbol AS crypto_symbol , -- Measures sp . price_usd , sp . volume_usd , sp . market_cap_usd FROM stg_prices sp JOIN dim_date dd ON sp . price_date = dd . full_date JOIN dim_crypto dc ON LOWER ( sp . coin_id ) = LOWER ( dc . coingecko_coin_id ) -- Robust join on CG ID { % if is_incremental () % } -- this filter will only be applied on an incremental run WHERE sp . price_date > ( SELECT MAX ( price_date ) FROM {{ this }} ) { % endif % }","title":"Fact Table Example"},{"location":"04_dbt_data_modeling/04_04_marts_layer/#relationship-to-bi-tools","text":"The tables created in the marts layer are the ones that Power BI (or any other BI tool) will connect to. The clear structure of facts and dimensions, well-defined relationships (enforced by dbt tests), and optimized performance make data analysis much more efficient and reliable. ``` docs/04_dbt_data_modeling/04_05_testing_and_quality.md ```markdown","title":"Relationship to BI Tools"},{"location":"04_dbt_data_modeling/04_04_marts_layer/#dbt-testing-and-data-quality","text":"Ensuring data quality is paramount for any analytics project. dbt provides a powerful and convenient framework for defining, executing, and managing data tests directly within your data transformation workflow. This helps build trust in the data and allows for early detection of issues.","title":"dbt Testing and Data Quality"},{"location":"04_dbt_data_modeling/04_04_marts_layer/#why-test-your-data","text":"Accuracy: Verify that calculations are correct and data reflects reality. Completeness: Ensure no critical data is missing. Consistency: Check that data is consistent across different tables and models. Integrity: Maintain referential integrity between related entities (e.g., foreign keys). Reliability: Build confidence that the data powering dashboards and decisions is trustworthy. Early Detection: Catch data issues early in the pipeline, before they impact downstream users or reports.","title":"Why Test Your Data?"},{"location":"04_dbt_data_modeling/04_04_marts_layer/#types-of-tests-in-dbt","text":"dbt supports several types of tests: Schema Tests (Generic Tests): These are predefined, out-of-the-box tests that can be applied to specific columns in your models. They are defined in .yml files, typically alongside your model definitions or in a dedicated schema.yml file within a model subdirectory. Common Schema Tests: unique : Asserts that all values in a column are unique. not_null : Asserts that there are no null values in a column. accepted_values : Asserts that all values in a column are within a specified list of allowed values. Example: tests: - accepted_values: values: ['BTC', 'ETH', 'DOGE'] relationships : Asserts referential integrity. It checks that every value in a column (foreign key) exists in a corresponding column of another model (primary key). Example: tests: - relationships: to: ref('dim_date') field: date_key Many more generic tests are available through packages like dbt-utils (e.g., equal_rowcount , expression_is_true ). Singular Data Tests (Custom Tests): These are custom SQL queries that you write to validate specific business logic or data conditions. They are stored as .sql files in the tests/ directory of your dbt project (often in tests/singular/ or tests/asserts/ ). A singular data test PASSES if the SQL query returns ZERO rows. If it returns one or more rows, the test FAILS, and those rows represent the records that violate the assertion. Example: A test to ensure that a transaction amount is always positive. sql -- tests/singular/assert_transaction_value_positive.sql SELECT transaction_pk, -- Or any relevant identifier transaction_amount_usd FROM {{ ref('fct_transactions') }} WHERE transaction_amount_usd < 0 Source Freshness (Specialized Test): Defined in sources.yml to monitor the timeliness of your raw data loads. Requires a loaded_at_field in your source table that indicates when the data was last updated. You can configure warn_after and error_after thresholds.","title":"Types of Tests in dbt"},{"location":"04_dbt_data_modeling/04_04_marts_layer/#implementation-strategy","text":"Test at Every Layer: Sources: Basic not_null on key identifiers, unique where expected, and freshness checks. Staging Models: unique and not_null on primary keys/business keys, accepted_values for categorical columns. Mart Models (Dimensions): unique and not_null on surrogate keys and natural keys. Mart Models (Facts): not_null on foreign keys and key measures. relationships tests to ensure all foreign keys correctly point to existing dimension records. Custom business logic tests (e.g., sum of debits equals sum of credits). Define Tests in .yml Files: For schema tests, co-locate them with your model definitions in .yml files. This keeps the model's contract (columns, descriptions, tests) in one place. Example: models/marts/core/fct_transactions.yml ```yaml version: 2 models: - name: fct_transactions description: \"Fact table containing individual blockchain transactions with their associated measures and dimensional foreign keys.\" columns: - name: transaction_pk # Assuming a surrogate key for the fact table description: \"Surrogate primary key for the transaction fact.\" tests: - unique - not_null - name: tx_hash description: \"The original transaction hash.\" tests: - not_null - name: price_date_key description: \"Foreign key to dim_date, representing the date of the transaction.\" tests: - not_null - relationships: to: ref('dim_date') field: date_key # Assuming 'date_key' is the PK in dim_date - name: crypto_key description: \"Foreign key to dim_cryptocurrency.\" tests: - not_null - relationships: to: ref('dim_cryptocurrency') field: crypto_key - name: fee_usd description: \"Transaction fee in USD.\" tests: - dbt_utils.expression_is_true: # Using a test from dbt_utils package expression: \">= 0\" # Fee should not be negative # This could also be a singular test for more complex logic # ... other columns and their tests ``` Leverage dbt Packages: Install and use packages like dbt-utils which provide a rich set of additional generic tests (e.g., dbt_utils.not_accepted_values , dbt_utils.equal_rowcount , dbt_utils.expression_is_true ). Add to packages.yml : ```yaml packages: package: dbt-labs/dbt_utils version: [\">=1.1.0\", \"<1.2.0\"] # Specify version range ``` Run dbt deps to install. Write Custom SQL Tests for Business Logic: If a validation cannot be expressed with a generic schema test, write a singular data test in the tests/ directory.","title":"Implementation Strategy"},{"location":"04_dbt_data_modeling/04_04_marts_layer/#running-tests","text":"Command: dbt test dbt test : Runs all tests (schema and singular). dbt test --select model_name : Runs tests defined on a specific model. dbt test --select source:source_name : Runs tests on a specific source. dbt test --select tag:my_tag : Runs tests associated with a specific tag. Output: dbt will report the status (PASS, FAIL, WARN, ERROR) of each test. For failing singular tests, it often shows the rows that failed the assertion. Integration with Airflow: Include a BashOperator task in your Airflow DAGs to execute dbt test after dbt run . The DAG can be configured to fail if any dbt tests fail, preventing bad data from being used downstream. ```python","title":"Running Tests"},{"location":"04_dbt_data_modeling/04_04_marts_layer/#in-airflow-dag","text":"task_dbt_test = BashOperator( task_id='dbt_run_tests', bash_command=f\"dbt test --project-dir {DBT_PROJECT_DIR} --profiles-dir {DBT_PROFILES_DIR}\", # ... )","title":"In Airflow DAG"},{"location":"04_dbt_data_modeling/04_04_marts_layer/#task_dbt_run-task_dbt_test","text":"``` By consistently applying tests throughout your dbt project, you significantly improve the reliability and trustworthiness of your data warehouse, providing a solid foundation for your BI and analytics efforts.","title":"task_dbt_run &gt;&gt; task_dbt_test"}]}